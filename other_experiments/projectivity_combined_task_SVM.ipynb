{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projectivity combined task SVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4-o99wPb5lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu6pel2nWGBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import linear_model, preprocessing\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from pandas import ExcelWriter\n",
        "from pandas import ExcelFile\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "# nltk.download('stopwords')\n",
        "# nltk.download()\n",
        "from nltk.corpus import stopwords "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DUffYSxtJl_",
        "colab_type": "code",
        "outputId": "806be8fd-bf5f-4f35-98c3-abe5541e1452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4116
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH011PZ0JWMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'a', 'an', 'the', 'and', 'of', 'for', 'with', 'about', 'between', 'into', 'through', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'under', 'here', 'there', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'so', 'than', 'too', 'very', 'just']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhIpVrntXle-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_train_val(train,val):\n",
        "  df= pd.read_csv(train,delimiter='\\t',encoding='utf-8')\n",
        "  tr_st=np.array(df['sentence'].values)\n",
        "  tr_label=np.array(df['label_combined'].values)\n",
        "  df= pd.read_csv(val,delimiter='\\t',encoding='utf-8')\n",
        "  val_st=np.array(df['sentence'].values)\n",
        "  val_label=np.array(df['label_combined'].values)\n",
        "  return tr_st,tr_label, val_st,val_label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxxZx8cfjN4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_test(test):\n",
        "  df= pd.read_csv(test,delimiter='\\t',encoding='utf-8')\n",
        "  ts_st=np.array(df['sentence'].values)\n",
        "  ts_label=np.array(df['label_combined'].values)\n",
        "  return ts_st,ts_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olOQd2m9VDBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentences(X):\n",
        "  documents = []\n",
        "  stemmer = WordNetLemmatizer()\n",
        "  for sen in range(0, len(X)):\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    document = re.sub(r'^\"', '', document)\n",
        "    document = re.sub(r'\"$', '', document)\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    document = document.lower()\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    #     document = ' '.join(document)\n",
        "    documents.append(document)\n",
        "  print(len(documents))\n",
        "  return documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQlVvKEebN01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_features(documents):\n",
        "  feature_docs=[]\n",
        "  for document in documents:\n",
        "    tokens=nltk.word_tokenize(document)\n",
        "    bigrams = list(nltk.bigrams(tokens))\n",
        "    #pos_bigrams = list(nltk.bigrams([pos for (word,pos) in nltk.pos_tag(tokens)]))\n",
        "    document=[word+' '+pos for (word,pos) in nltk.pos_tag(tokens)]#+' '+pos\n",
        "    document.extend([w1+'_'+w2 for (w1,w2) in bigrams])\n",
        "    #     document.extend([w1+'_'+w2 for (w1,w2) in pos_bigrams])\n",
        "    #     document.extend([pos for (word,pos) in nltk.pos_tag(tokens)])\n",
        "    #     print(document)\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    feature_docs.append(document)\n",
        "  return feature_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DHUdtgJXlKG",
        "colab_type": "code",
        "outputId": "5c6b679b-0091-4818-98ed-08497ecc8afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# documents = []\n",
        "# stemmer = WordNetLemmatizer()\n",
        "# for sen in range(0, len(X)):\n",
        "#   document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "#   document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "#   document = re.sub(r'^\"', '', document)\n",
        "#   document = re.sub(r'\"$', '', document)\n",
        "#   document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "#   document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "#   document = re.sub(r'^b\\s+', '', document)\n",
        "#   document = document.lower()\n",
        "#   #     document = document.split()\n",
        "#   #     document = [stemmer.lemmatize(word) for word in document]\n",
        "#   #     document = ' '.join(document)\n",
        "#   documents.append(document)\n",
        "# print(len(documents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMYF3QAbD9pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "\n",
        "with open(\"glove.6B.300d.txt\", \"r\", encoding='utf-8', errors='ignore') as lines:\n",
        "    w2v = {line.split()[0]: np.array(list(map(float, line.split()[1:]))) for line in lines}\n",
        "print(w2v['of'])\n",
        "# def readWordvectors(wordvectorfile):\n",
        "#   wordvectors = {}\n",
        "#   vectorsize = 0\n",
        "#   if \".gz\" in wordvectorfile:\n",
        "#     f = gzip.open(wordvectorfile, 'r')\n",
        "#   else:\n",
        "#     f = open(wordvectorfile, 'r',encoding='latin-1')\n",
        "#   count = 0\n",
        "#   for line in f:\n",
        "#     if count == 0:\n",
        "#       count += 1\n",
        "#       continue\n",
        "#     parts = line.split()\n",
        "#     word = parts[0]\n",
        "#     parts.pop(0)\n",
        "#     wordvectors[word] = parts\n",
        "#     vectorsize = len(parts)\n",
        "#   f.close()\n",
        "#   return [wordvectors, vectorsize]\n",
        "# w2v,vectorsize= readWordvectors('vecs.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ5AmsWY1K2v",
        "colab_type": "code",
        "outputId": "2be4fd50-a104-41f6-85b3-b8a9d581d133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(w2v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahJw58VtkqNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os    \n",
        "from chardet import detect\n",
        "\n",
        "def get_encoding_type(file):\n",
        "    with open(file, 'rb') as f:\n",
        "        rawdata = f.read().decode('latin-1')\n",
        "    return detect(rawdata)['encoding']\n",
        "\n",
        "from_codec = get_encoding_type(\"vecs.txt\")\n",
        "print(from_codec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVvOgipmDhes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(list(w2v.values())[1])\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.sum([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhx8SsoqRXfE",
        "colab_type": "code",
        "outputId": "e33b1317-e472-4050-b29a-2a918006fe37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(list(w2v.values())[1])\n",
        "\n",
        "# list(w2v.keys())[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAYfVPXZi9-e",
        "colab_type": "code",
        "outputId": "096de173-483f-4ab0-ddd3-4947fbac2190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDi5ALdwi_BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapath = 'gdrive/My Drive/data/'\n",
        "trainfile=datapath+'train_combined.tsv'\n",
        "valfile=datapath+'val_combined.tsv'\n",
        "testfile=datapath+'test_combined.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjfiO4FeJz6",
        "colab_type": "code",
        "outputId": "98204489-7431-45b4-b3ad-7e38cb54c17b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#loading train and validation data\n",
        "# trainfile='train.txt'\n",
        "# valfile='validation.txt'\n",
        "tr_st,tr_label, val_st,val_label=load_train_val(trainfile,valfile)\n",
        "\n",
        "#preprocessing\n",
        "tr_st = preprocess_sentences(tr_st)\n",
        "val_st = preprocess_sentences(val_st)\n",
        "\n",
        "#feature extraction\n",
        "tr_st = sent_to_features(tr_st)\n",
        "val_st = sent_to_features(val_st)\n",
        "\n",
        "print('Train size: ',len(tr_st))\n",
        "print('Val size: ',len(val_st))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1776\n",
            "592\n",
            "Train size:  1776\n",
            "Val size:  592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUwpVDvpCT7n",
        "colab_type": "code",
        "outputId": "8904109d-046f-4680-b1c8-b18ee1afbc85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tr_st[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "friends NNS and CC adversaries NNS abroad RB were VBD asking VBG whether IN america NN had VBD lost VBN its PRP$ nerve NN friends_and and_adversaries adversaries_abroad abroad_were were_asking asking_whether whether_america america_had had_lost lost_its its_nerve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZoEsHeU9xbq",
        "colab_type": "code",
        "outputId": "6e343c02-da08-4c38-bb44-635d204d1d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'a', 'an', 'the', 'and', 'of', 'for', 'with', 'about', 'between', 'into', 'through', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'under', 'here', 'there', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'so', 'than', 'too', 'very', 'just']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMO_e_OTXn3u",
        "colab_type": "code",
        "outputId": "fbcaf90f-6b13-416f-cca0-eb6ced5c7aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  \n",
        "vectorizer = CountVectorizer( stop_words=stop_words)  #, stop_words=stopwords.words('english')\n",
        "documents = np.concatenate((tr_st,val_st))\n",
        "print(len(documents))\n",
        "vectorizer.fit(documents)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "        ngram_range=(1, 1), preprocessor=None,\n",
              "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', '...t', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'so', 'than', 'too', 'very', 'just'],\n",
              "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "        tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en7u9Sbpq3Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_st = vectorizer.transform(tr_st)\n",
        "val_st = vectorizer.transform(val_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Ugxes-Xp_W",
        "colab_type": "code",
        "outputId": "d21725cf-26f7-4a3f-d1ec-86483297a21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tr_st.shape)\n",
        "# print(X[0][:1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1776, 39395)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8t3ksPKaWWA",
        "colab_type": "code",
        "outputId": "daed06c9-966b-4b9b-9a12-677f729f05f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "tfidfconverter = TfidfVectorizer()#min_df=2, max_df=0.8, stop_words=stopwords.words('english')\n",
        "documents = np.concatenate((tr_st,val_st))\n",
        "print(len(documents))\n",
        "tfidfconverter.fit(documents) \n",
        "tr_st = tfidfconverter.transform(tr_st)\n",
        "val_st = tfidfconverter.transform(val_st)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2RoOKcNaZQr",
        "colab_type": "code",
        "outputId": "1d719aa2-27cd-4f5b-9ee5-a61dd329d0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tr_st.shape)\n",
        "# print(Xt[0][:1000])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1776, 39471)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53itS7qvRgnF",
        "colab_type": "code",
        "outputId": "b0e5953a-a0ee-4add-d707-12f7e5c6ab67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "meanevconverter= MeanEmbeddingVectorizer(w2v)\n",
        "tr_st = meanevconverter.transform(tr_st)\n",
        "print('tranformed tr data: ', tr_st.shape)\n",
        "val_st = meanevconverter.transform(val_st)\n",
        "print('tranformed val data', val_st.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tranformed tr data:  (1776, 300)\n",
            "tranformed val data (592, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOmHH25YhJZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tr_st[20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8QRMPpBkIYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import train_test_split  \n",
        "# Xtr, Xts, ytr, yts = train_test_split(X, yf, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOqLdeuWj4MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "svc = svm.SVC(probability=False, kernel=\"linear\", C=2.8, gamma=.0073,verbose=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fSdNcx6j5LZ",
        "colab_type": "code",
        "outputId": "f89b4c4f-57b1-4232-824d-b78d9967ec78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "svc.fit(tr_st,tr_label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=2.8, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.0073, kernel='linear',\n",
              "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGeCYbYnlLby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat_ts = svc.predict(val_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCpFiToN-mjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat_tr = svc.predict(tr_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWLVkclj-sF2",
        "colab_type": "code",
        "outputId": "d59d1bff-40fa-4e80-c7b5-9aa9c483e03c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc = np.mean(yhat_tr == tr_label)\n",
        "print('Training Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuaracy = 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVJLNXdqj7eB",
        "colab_type": "code",
        "outputId": "c8082b6a-fc04-47ca-ac8c-8e1608314348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc = np.mean(yhat_ts == val_label)\n",
        "print('Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuaracy = 0.657095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWMtkPSHnGcu",
        "colab_type": "code",
        "outputId": "98b9c6a8-487e-4c49-d2be-631718f632df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['class 0', 'class 1','class 2','class 3']\n",
        "print(classification_report(val_label, yhat_ts, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.41      0.44      0.42        73\n",
            "     class 1       0.70      0.62      0.66       183\n",
            "     class 2       0.70      0.85      0.77       279\n",
            "     class 3       0.44      0.12      0.19        57\n",
            "\n",
            "   micro avg       0.66      0.66      0.66       592\n",
            "   macro avg       0.56      0.51      0.51       592\n",
            "weighted avg       0.64      0.66      0.64       592\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZLnofLy3K_g",
        "colab_type": "code",
        "outputId": "111dfd69-eeb7-4ff1-dfda-2b9d3bb29073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(\"Future:\")\n",
        "unique, counts = np.unique(np.concatenate((tr_ft,val_ft)), return_counts=True)\n",
        "print(\"Total: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(tr_ft, return_counts=True)\n",
        "print(\"Train: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(val_ft, return_counts=True)\n",
        "print(\"Test: \",dict(zip(unique, counts)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Future:\n",
            "Total:  {0.0: 1012, 1.0: 1356}\n",
            "Train:  {0.0: 756, 1.0: 1020}\n",
            "Test:  {0.0: 256, 1.0: 336}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPIWXyWpwEIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C_test = [0.1,1,2.5,2.8,5,10]\n",
        "gam_test = [0.001,0.01,0.1]\n",
        "\n",
        "nC = len(C_test)\n",
        "ngam = len(gam_test)\n",
        "acc = np.zeros((nC,ngam))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-zgi74iwKz7",
        "colab_type": "code",
        "outputId": "45eb963c-c2f2-455c-af4f-7d7db6888cc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "# Xtr, Xts, ytr, yts = train_test_split(X, yf, test_size=0.3, random_state=42)\n",
        "\n",
        "# TODO:  Print the accuracy matrix\n",
        "for index_C , C in enumerate(C_test):\n",
        "  for index_G , gamma in enumerate(gam_test):\n",
        "    svc2 = svm.SVC(probability=True, kernel=\"linear\", C=C, gamma=gamma, verbose=10)\n",
        "    svc2.fit(tr_st,tr_label)\n",
        "    yhat_testing_cross = svc2.predict(val_st)\n",
        "    acc[index_C , index_G ] = np.mean(yhat_testing_cross == val_label)\n",
        "    print(C,gamma)\n",
        "    print('Acc', acc[index_C,index_G])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]0.1 0.001\n",
            "Acc 0.47128378378378377\n",
            "[LibSVM]0.1 0.01\n",
            "Acc 0.47128378378378377\n",
            "[LibSVM]0.1 0.1\n",
            "Acc 0.47128378378378377\n",
            "[LibSVM]1 0.001\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]1 0.01\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]1 0.1\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]2.5 0.001\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]2.5 0.01\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]2.5 0.1\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]2.8 0.001\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]2.8 0.01\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]2.8 0.1\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]5 0.001\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]5 0.01\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]5 0.1\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]10 0.001\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]10 0.01\n",
            "Acc 0.6570945945945946\n",
            "[LibSVM]10 0.1\n",
            "Acc 0.6570945945945946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyYGDe8xwnvJ",
        "colab_type": "code",
        "outputId": "27a65bef-2bb1-4342-ce95-1055706e6c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "optimal_index_C , optimal_index_Gamma = np.unravel_index(acc.argmax(),acc.shape)\n",
        "\n",
        "print(\"Maximum Accuracy\",acc[optimal_index_C][optimal_index_Gamma])\n",
        "opt_c_f = C_test[optimal_index_C]\n",
        "opt_g_f = gam_test[optimal_index_Gamma]\n",
        "print(\"Optimal C:\", C_test[optimal_index_C])\n",
        "print('Optimal Gamma:' , gam_test[optimal_index_Gamma])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum Accuracy 0.8023648648648649\n",
            "Optimal C: 0.1\n",
            "Optimal Gamma: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1x5L7U3l1jC",
        "colab_type": "code",
        "outputId": "d3a8e809-b9ed-432f-ef17-115669ef3b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "testfile='test.txt'\n",
        "ts_st,ts_ft,ts_pt=load_test(testfile)\n",
        "\n",
        "#preprocessing\n",
        "ts_st = preprocess_sentences(ts_st)\n",
        "\n",
        "#feature extraction\n",
        "ts_st = sent_to_features(ts_st)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_bSNz61cG5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ts_st = vectorizer.transform(ts_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWd2VxQ4bmy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts_st = tfidfconverter.transform(ts_st)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LmzFJ32emgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_val_st = vectorizer.transform(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM8QC_ikeneN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_val_st = tfidfconverter.transform(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "287lkbHfmeMS",
        "colab_type": "code",
        "outputId": "0a97f4e3-7505-4a12-bf80-79a3f86e4d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "documents = np.concatenate((tr_st,val_st))\n",
        "print(len(documents))\n",
        "tr_val_st = meanevconverter.transform(documents)\n",
        "ts_st = meanevconverter.transform(ts_st)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ll4LYuczULY",
        "colab_type": "code",
        "outputId": "9bbaa20f-dcc7-4467-aca7-6afabbbd9ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "\n",
        "svc2 = svm.SVC(probability=True, kernel=\"linear\", C=0.1, gamma=0.001, verbose=10)\n",
        "print(tr_val_st.shape,tr_ft.shape,val_ft.shape)\n",
        "svc2.fit(tr_val_st,np.concatenate((tr_ft,val_ft)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2368, 39395) (1776,) (592,)\n",
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
              "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0jWpS90zX0c",
        "colab_type": "code",
        "outputId": "f8624725-a911-4393-b9c8-bf379efc9132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yhat_testing = svc2.predict(ts_st)\n",
        "accu = np.mean(yhat_testing == ts_ft)\n",
        "print('Test Accuracy: ',accu)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.806070826306914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFKwHCsz0TnC",
        "colab_type": "code",
        "outputId": "0417f14a-3f81-4280-cfa9-82876ca6f0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(ts_ft, yhat_testing, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.00      0.00      0.00       293\n",
            "     class 1       0.51      1.00      0.67       300\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       593\n",
            "   macro avg       0.25      0.50      0.34       593\n",
            "weighted avg       0.26      0.51      0.34       593\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}