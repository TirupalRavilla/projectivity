{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projectivity SVM with embeddings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu6pel2nWGBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "647897cb-9b5c-43d5-bead-78f3b97d38a7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import linear_model, preprocessing\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from pandas import ExcelWriter\n",
        "from pandas import ExcelFile\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "# nltk.download('stopwords')\n",
        "# nltk.download()\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DUffYSxtJl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3713
        },
        "outputId": "b1f900f6-ae5e-4b27-ab57-e4c17bd95567"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('all')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Error downloading 'word2vec_sample' from\n",
            "[nltk_data]    |     <https://raw.githubusercontent.com/nltk/nltk_data\n",
            "[nltk_data]    |     /gh-pages/packages/models/word2vec_sample.zip>:\n",
            "[nltk_data]    |     HTTP Error 503: first byte timeout\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBGoLlU_WIi4",
        "colab_type": "code",
        "outputId": "778c0337-0f30-446e-ddfc-0444009e32a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# data_path='./'  \n",
        "# os.chdir(data_path)\n",
        "files=glob.glob('*.txt')\n",
        "\n",
        "sents=[]\n",
        "tagsP=[]\n",
        "tagsF=[]\n",
        "emptyin=[]\n",
        "for f in files:\n",
        "    file = open(f)\n",
        "    df= pd.read_csv(file,delimiter='\\t',encoding='utf-8')\n",
        "    emptyin=df[df['Past'].isnull()].index.tolist()\n",
        "    st=np.delete(np.array(df['sentence'].values),emptyin)\n",
        "    ft=np.delete(np.array(df['Future'].values),emptyin)\n",
        "    pt=np.delete(np.array(df['Past'].values),emptyin)\n",
        "    sents.extend(list(st))\n",
        "    tagsF.extend(list(ft))\n",
        "    tagsP.extend(list(pt))\n",
        "    print(emptyin)\n",
        "X=np.asarray(sents)\n",
        "yf=np.asarray(tagsF)\n",
        "yp=np.asarray(tagsP)\n",
        "print(len(X))\n",
        "print(len(yf))\n",
        "print(len(yp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[103, 266, 280, 301, 314, 392, 412, 474, 664, 685, 688, 778, 875, 926, 975]\n",
            "[49, 82, 195, 258, 533, 541, 577, 648, 726, 942]\n",
            "[51, 90, 214, 273, 304, 404, 458, 511, 527, 640, 664, 769, 911, 995]\n",
            "2961\n",
            "2961\n",
            "2961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vMJZxPyQ6iF",
        "colab_type": "code",
        "outputId": "1d1f74a3-d0dd-490f-c0ff-aa1123730911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f='projectivity.txt'\n",
        "file=open(f)\n",
        "df= pd.read_csv(file,delimiter='\\t',encoding='utf-8')\n",
        "emptyin=df[df['Past'].isnull()].index\n",
        "dt=df.loc[:, 'id':'Future']\n",
        "dt=dt.drop(emptyin)\n",
        "print(len(dt))\n",
        "# dt=np.delete(np.array(df.loc[:, 'A':'E'].values),emptyin)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8l5b-sHCCF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data_and_store(df):\n",
        "  train, validation, test = np.split(df, [int(.6*len(df)), int(.8*len(df))])\n",
        "  train.to_csv('train', sep='\\t')\n",
        "  validation.to_csv('validation', sep='\\t')\n",
        "  test.to_csv('test', sep='\\t')\n",
        "  print('Done!')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utar4lc9alxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_data_and_store(dt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH011PZ0JWMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'a', 'an', 'the', 'and', 'of', 'for', 'with', 'about', 'between', 'into', 'through', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'under', 'here', 'there', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'so', 'than', 'too', 'very', 'just']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhIpVrntXle-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_train_val(train,val):\n",
        "  df= pd.read_csv(train,delimiter='\\t',encoding='utf-8')\n",
        "  tr_st=np.array(df['sentence'].values)\n",
        "  tr_ft=np.array(df['Future'].values)\n",
        "  tr_pt=np.array(df['Past'].values)\n",
        "  df= pd.read_csv(val,delimiter='\\t',encoding='utf-8')\n",
        "  val_st=np.array(df['sentence'].values)\n",
        "  val_ft=np.array(df['Future'].values)\n",
        "  val_pt=np.array(df['Past'].values)\n",
        "  return tr_st,tr_ft,tr_pt, val_st,val_ft,val_pt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxxZx8cfjN4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_test(test):\n",
        "  df= pd.read_csv(test,delimiter='\\t',encoding='utf-8')\n",
        "  ts_st=np.array(df['sentence'].values)\n",
        "  ts_ft=np.array(df['Future'].values)\n",
        "  ts_pt=np.array(df['Past'].values)\n",
        "  return ts_st,ts_ft,ts_pt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olOQd2m9VDBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentences(X):\n",
        "  documents = []\n",
        "  stemmer = WordNetLemmatizer()\n",
        "  for sen in range(0, len(X)):\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    document = re.sub(r'^\"', '', document)\n",
        "    document = re.sub(r'\"$', '', document)\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    document = document.lower()\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    #     document = ' '.join(document)\n",
        "    documents.append(document)\n",
        "  print(len(documents))\n",
        "  return documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQlVvKEebN01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_features(documents):\n",
        "  feature_docs=[]\n",
        "  for document in documents:\n",
        "    tokens=nltk.word_tokenize(document)\n",
        "    bigrams = list(nltk.bigrams(tokens))\n",
        "    #pos_bigrams = list(nltk.bigrams([pos for (word,pos) in nltk.pos_tag(tokens)]))\n",
        "    document=[word for (word,pos) in nltk.pos_tag(tokens)]#+' '+pos\n",
        "#     document.extend([w1+'_'+w2 for (w1,w2) in bigrams])\n",
        "    #     document.extend([w1+'_'+w2 for (w1,w2) in pos_bigrams])\n",
        "    #     document.extend([pos for (word,pos) in nltk.pos_tag(tokens)])\n",
        "    #     print(document)\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    feature_docs.append(document)\n",
        "  return feature_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DHUdtgJXlKG",
        "colab_type": "code",
        "outputId": "5c6b679b-0091-4818-98ed-08497ecc8afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# documents = []\n",
        "# stemmer = WordNetLemmatizer()\n",
        "# for sen in range(0, len(X)):\n",
        "#   document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "#   document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "#   document = re.sub(r'^\"', '', document)\n",
        "#   document = re.sub(r'\"$', '', document)\n",
        "#   document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "#   document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "#   document = re.sub(r'^b\\s+', '', document)\n",
        "#   document = document.lower()\n",
        "#   #     document = document.split()\n",
        "#   #     document = [stemmer.lemmatize(word) for word in document]\n",
        "#   #     document = ' '.join(document)\n",
        "#   documents.append(document)\n",
        "# print(len(documents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMYF3QAbD9pi",
        "colab_type": "code",
        "outputId": "e42de647-65ae-4a88-fad7-55a33299db40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "# import numpy as np\n",
        "\n",
        "with open(datapath+\"glove.6B.300d.txt\", \"r\", encoding='utf-8', errors='ignore') as lines:\n",
        "    w2v = {line.split()[0]: np.array(list(map(float, line.split()[1:]))) for line in lines}\n",
        "print(w2v['of'])\n",
        "# def readWordvectors(wordvectorfile):\n",
        "#   wordvectors = {}\n",
        "#   vectorsize = 0\n",
        "#   if \".gz\" in wordvectorfile:\n",
        "#     f = gzip.open(wordvectorfile, 'r')\n",
        "#   else:\n",
        "#     f = open(wordvectorfile, 'r',encoding='latin-1')\n",
        "#   count = 0\n",
        "#   for line in f:\n",
        "#     if count == 0:\n",
        "#       count += 1\n",
        "#       continue\n",
        "#     parts = line.split()\n",
        "#     word = parts[0]\n",
        "#     parts.pop(0)\n",
        "#     wordvectors[word] = parts\n",
        "#     vectorsize = len(parts)\n",
        "#   f.close()\n",
        "#   return [wordvectors, vectorsize]\n",
        "# w2v,vectorsize= readWordvectors('vecs.txt')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-7.6947e-02 -2.1211e-02  2.1271e-01 -7.2232e-01 -1.3988e-01 -1.2234e-01\n",
            " -1.7521e-01  1.2137e-01 -7.0866e-02 -1.5721e+00 -2.2464e-01  4.2690e-02\n",
            " -4.0180e-01  2.1006e-01  1.4288e-02  4.1628e-01  1.7165e-02  7.1732e-02\n",
            "  6.9246e-03  1.8107e-01 -1.5412e-01  1.4933e-01 -3.0493e-02  2.9918e-01\n",
            "  2.9479e-02 -3.6147e-02 -6.1125e-02  8.3918e-02 -1.2398e-01 -1.0077e-01\n",
            " -5.4142e-03  3.3710e-01 -2.5612e-01  4.4388e-01 -6.8922e-01  1.8020e-01\n",
            "  3.4898e-01 -5.2284e-02 -2.6226e-01 -4.7109e-01  2.1647e-01 -4.0020e-01\n",
            " -4.9986e-02  1.1376e-02  5.4994e-01 -2.2791e-01  9.5873e-02  4.7693e-01\n",
            " -5.6727e-02 -1.7895e-01  1.1756e-01  1.4662e-01  4.8948e-02  1.3587e-01\n",
            " -9.3821e-02  4.5968e-01 -3.2062e-01  2.9911e-01  2.0656e-01 -1.8503e-01\n",
            " -2.7690e-01 -2.2545e-02  7.0698e-01 -2.3815e-01  1.6437e-01 -5.5044e-01\n",
            " -1.0615e-03  1.2266e-01  1.1898e-01  2.3985e-01  2.9815e-01  1.3207e-02\n",
            "  1.6316e-01 -6.1334e-01 -3.7051e-01  1.9444e-01 -1.3621e-01 -3.0426e-01\n",
            " -3.7715e-01  6.5299e-02 -1.5995e-01 -5.6516e-01  7.4696e-02  4.0184e-01\n",
            "  1.9328e-01  4.1802e-02  2.0572e-01  2.8971e-01  3.4783e-01  3.3873e-01\n",
            " -1.0052e-01 -1.6397e-01 -1.5236e-01 -8.6815e-02  3.6522e-01  1.4969e-01\n",
            " -4.0859e-01  2.3106e-01  1.7162e-01 -6.0545e-01  8.6019e-02  3.7043e-01\n",
            "  1.7937e-01 -4.0282e-01 -6.2471e-01 -5.5919e-02  1.5092e-01  1.2554e-01\n",
            " -4.5344e-01  3.4417e-01  4.0042e-01 -4.9512e-02 -2.9969e-01 -3.1761e-01\n",
            "  3.0023e-01  9.0029e-02  3.1060e-01 -3.3077e-02 -2.1995e-01 -4.0396e-01\n",
            " -3.4443e-01 -2.1248e-01 -3.7636e-01  2.1835e-01 -1.7850e-01 -1.7261e-01\n",
            "  1.6391e-01  2.2753e-01  2.6860e-01  5.7541e-01 -1.4912e-01  2.0413e-01\n",
            "  2.2187e-01 -2.7014e-01  6.8253e-02  2.9115e-01 -6.7943e-02  1.0623e-01\n",
            " -1.6281e-01  1.9939e-01 -4.8613e-01  3.5688e-02 -1.2373e-01  1.3707e-01\n",
            "  3.3359e-01 -1.2713e-01 -3.1711e-01 -1.3962e-01 -4.2880e-02 -1.4614e-03\n",
            "  7.6883e-01 -4.1705e-01 -9.2911e-02  1.6315e-01  2.9202e-01  1.2119e-01\n",
            " -7.6683e-02  1.4131e-01 -9.3406e-02 -4.2796e-02  1.3738e-01  1.4278e-02\n",
            "  1.1918e-01 -3.4215e-01 -1.9076e-01 -1.2499e-01  2.4648e-01  4.2259e-01\n",
            "  9.1966e-02  4.5351e-01  1.4437e-01  1.8780e-01 -8.5876e-01  5.9621e-02\n",
            " -3.2242e-01  2.8627e-01  1.2427e-01  9.0984e-03 -1.8910e-01  1.6638e-01\n",
            "  9.9881e-02 -4.8553e-02 -2.6257e-02  9.9904e-02  1.2406e-01 -1.5416e-02\n",
            " -2.9707e-01 -4.0440e-01 -1.7258e-01  3.6468e-01 -1.4118e-02 -1.1889e-01\n",
            " -1.1686e-01 -1.4124e-01  2.8012e-01  6.7644e-02  1.4850e-01 -3.5702e-01\n",
            "  2.9626e-01  3.6004e-01  1.0190e+00 -6.7307e-02 -1.1588e-01 -2.1780e-01\n",
            "  7.0191e-02  2.3154e-01 -1.3849e-01  2.6441e-01  2.8742e-01  1.9410e-01\n",
            " -6.0504e-03  4.4105e-01  1.2416e-01 -2.7745e-01 -2.5729e-01  1.0992e-01\n",
            "  1.8362e-01 -3.4522e-01 -2.1861e-01 -1.8825e-01 -3.7454e-02 -2.0862e-01\n",
            " -2.5216e-01  6.0842e-02  6.8595e-02  1.0275e-01  1.0745e-01 -6.1288e-02\n",
            "  1.9725e-01 -2.7739e-01 -2.2559e-02  5.2794e-02 -2.4083e-01  9.1990e-02\n",
            "  3.0959e-01  5.4999e-02  6.3676e-02 -8.7357e-02 -3.4495e-01  2.2793e-01\n",
            " -4.2405e-01  2.4536e-01  5.5708e-01  1.9126e-01 -7.9700e-01 -2.0480e-01\n",
            "  3.2545e-01  9.2350e-02  8.4791e-02 -1.6433e-01 -6.6568e-02 -9.9249e-02\n",
            "  3.1526e-01 -4.4465e-01  8.7281e-02  3.2880e-01 -1.7809e-02 -2.3855e-01\n",
            " -1.2848e-01  4.1509e-02  4.6728e-01  4.8214e-01  1.0548e-01  6.5805e-02\n",
            "  6.7221e-02  1.3321e-01 -2.7856e-01  1.5532e-02  3.0026e-01  3.8748e-01\n",
            " -1.4401e-01 -1.6131e-01  1.7678e-01  1.6448e-01 -3.2440e-01  7.9370e-03\n",
            " -2.2836e+00  9.6945e-02  6.6131e-01  1.6857e-01 -2.8877e-02 -1.0791e-01\n",
            " -2.7445e-02 -2.5695e-01  4.6686e-02  2.3087e-01 -7.6458e-02  2.7127e-01\n",
            "  2.5185e-01  5.4947e-02 -3.6673e-01 -3.8603e-01  3.0290e-01  1.5747e-02\n",
            "  3.4036e-01  4.7841e-01  6.8617e-02  1.8351e-01 -2.9183e-01 -4.6533e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ5AmsWY1K2v",
        "colab_type": "code",
        "outputId": "f5571436-231b-4dc1-876e-32b7c72af748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(w2v)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahJw58VtkqNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os    \n",
        "from chardet import detect\n",
        "\n",
        "def get_encoding_type(file):\n",
        "    with open(file, 'rb') as f:\n",
        "        rawdata = f.read().decode('latin-1')\n",
        "    return detect(rawdata)['encoding']\n",
        "\n",
        "from_codec = get_encoding_type(\"vecs.txt\")\n",
        "print(from_codec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVvOgipmDhes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(list(w2v.values())[1])\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.sum([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhx8SsoqRXfE",
        "colab_type": "code",
        "outputId": "b8b61bce-f1f2-46f8-8cf9-c0d32932385b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(list(w2v.values())[1])\n",
        "\n",
        "# list(w2v.keys())[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KDM8T2_QaN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "65df4ea5-48aa-4e59-c4ed-62d21af5df0d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "henspAPcQbKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapath = 'gdrive/My Drive/data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjfiO4FeJz6",
        "colab_type": "code",
        "outputId": "1707ede7-5208-47c1-be51-a18b83bcd3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#loading train and validation data\n",
        "trainfile=datapath+'train.txt'\n",
        "valfile=datapath+'validation.txt'\n",
        "tr_st,tr_ft,tr_pt, val_st,val_ft,val_pt=load_train_val(trainfile,valfile)\n",
        "\n",
        "#preprocessing\n",
        "tr_st = preprocess_sentences(tr_st)\n",
        "val_st = preprocess_sentences(val_st)\n",
        "\n",
        "#feature extraction\n",
        "tr_st = sent_to_features(tr_st)\n",
        "val_st = sent_to_features(val_st)\n",
        "\n",
        "print('Train size: ',len(tr_st))\n",
        "print('Val size: ',len(val_st))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1776\n",
            "592\n",
            "Train size:  1776\n",
            "Val size:  592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUwpVDvpCT7n",
        "colab_type": "code",
        "outputId": "be1ae797-05d5-4b7a-f3d3-f65e995311f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tr_st[1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "friends and adversaries abroad were asking whether america had lost its nerve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZoEsHeU9xbq",
        "colab_type": "code",
        "outputId": "89436013-97d7-4170-d0a7-240f023577d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'a', 'an', 'the', 'and', 'of', 'for', 'with', 'about', 'between', 'into', 'through', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'under', 'here', 'there', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'so', 'than', 'too', 'very', 'just']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMO_e_OTXn3u",
        "colab_type": "code",
        "outputId": "aa2e4525-2add-457f-8a1a-3ab5051c98d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  \n",
        "vectorizer = CountVectorizer( stop_words=stop_words)  #, stop_words=stopwords.words('english')\n",
        "documents = np.concatenate((tr_st,val_st))\n",
        "print(len(documents))\n",
        "vectorizer.fit(documents)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "        ngram_range=(1, 1), preprocessor=None,\n",
              "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', '...t', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'so', 'than', 'too', 'very', 'just'],\n",
              "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "        tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en7u9Sbpq3Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_st = vectorizer.transform(tr_st)\n",
        "val_st = vectorizer.transform(val_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Ugxes-Xp_W",
        "colab_type": "code",
        "outputId": "d21725cf-26f7-4a3f-d1ec-86483297a21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tr_st.shape)\n",
        "# print(X[0][:1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1776, 39395)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8t3ksPKaWWA",
        "colab_type": "code",
        "outputId": "f113c9ad-bc63-43db-f7c6-0c0deb9bf5c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "tfidfconverter = TfidfVectorizer()#min_df=2, max_df=0.8, stop_words=stopwords.words('english')\n",
        "documents = np.concatenate((tr_st,val_st))\n",
        "print(len(documents))\n",
        "tfidfconverter.fit(documents) \n",
        "tr_st = tfidfconverter.transform(tr_st)\n",
        "val_st = tfidfconverter.transform(val_st)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2RoOKcNaZQr",
        "colab_type": "code",
        "outputId": "506a0089-3dd9-4ab1-cb95-23230dcbb3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tr_st.shape)\n",
        "# print(Xt[0][:1000])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1776, 39471)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53itS7qvRgnF",
        "colab_type": "code",
        "outputId": "874bac97-e831-43bd-afc7-17af7e230538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "meanevconverter= MeanEmbeddingVectorizer(w2v)\n",
        "tr_st = meanevconverter.transform(tr_st)\n",
        "print('tranformed tr data: ', tr_st.shape)\n",
        "val_st = meanevconverter.transform(val_st)\n",
        "print('tranformed val data', val_st.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tranformed tr data:  (1776, 300)\n",
            "tranformed val data (592, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOmHH25YhJZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tr_st[20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8QRMPpBkIYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import train_test_split  \n",
        "# Xtr, Xts, ytr, yts = train_test_split(X, yf, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOqLdeuWj4MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "svc = svm.SVC(probability=False, kernel=\"linear\", C=2.8, gamma=.0073,verbose=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fSdNcx6j5LZ",
        "colab_type": "code",
        "outputId": "f4d7099e-cba2-40ea-eab5-5bd22a6ecb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "svc.fit(tr_st,tr_ft)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=2.8, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.0073, kernel='linear',\n",
              "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGeCYbYnlLby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat_ts = svc.predict(val_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCpFiToN-mjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat_tr = svc.predict(tr_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWLVkclj-sF2",
        "colab_type": "code",
        "outputId": "038dcb07-d3fa-4f00-ade5-4661d3dd3790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc = np.mean(yhat_tr == tr_ft)\n",
        "print('Training Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuaracy = 0.643581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVJLNXdqj7eB",
        "colab_type": "code",
        "outputId": "483a439f-e1bb-4def-aff8-7d63dc5c8da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc = np.mean(yhat_ts == val_ft)\n",
        "print('Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuaracy = 0.636824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWMtkPSHnGcu",
        "colab_type": "code",
        "outputId": "4d9eaef3-6592-49a0-94f5-3ad8191aeb3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(val_ft, yhat_ts, target_names=target_names))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.62      0.42      0.50       256\n",
            "     class 1       0.65      0.80      0.71       336\n",
            "\n",
            "   micro avg       0.64      0.64      0.64       592\n",
            "   macro avg       0.63      0.61      0.61       592\n",
            "weighted avg       0.63      0.64      0.62       592\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZLnofLy3K_g",
        "colab_type": "code",
        "outputId": "111dfd69-eeb7-4ff1-dfda-2b9d3bb29073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(\"Future:\")\n",
        "unique, counts = np.unique(np.concatenate((tr_ft,val_ft)), return_counts=True)\n",
        "print(\"Total: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(tr_ft, return_counts=True)\n",
        "print(\"Train: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(val_ft, return_counts=True)\n",
        "print(\"Test: \",dict(zip(unique, counts)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Future:\n",
            "Total:  {0.0: 1012, 1.0: 1356}\n",
            "Train:  {0.0: 756, 1.0: 1020}\n",
            "Test:  {0.0: 256, 1.0: 336}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N7kXxvjvWZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Xtr, Xts, ytrp, ytsp = train_test_split(X, yp, test_size=0.3, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8gI3gWYkJ5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svcp = svm.SVC(probability=True, kernel=\"linear\", C=2.8, gamma=.0073,verbose=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORc9bdqhviuc",
        "colab_type": "code",
        "outputId": "d022ef03-20c8-4284-9c9b-3016b569cbec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "svcp.fit(tr_st,tr_pt)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=2.8, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.0073, kernel='linear',\n",
              "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8td74YeCvu7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat_tsp = svcp.predict(val_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHUgtXX8-4m2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat_trp = svcp.predict(tr_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTFb9QVr-7nb",
        "colab_type": "code",
        "outputId": "3dae05c8-1c47-4055-962e-7fd0f1996735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc = np.mean(yhat_trp == tr_pt)\n",
        "print('Training Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuaracy = 0.681306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anOCYdy4vzsg",
        "colab_type": "code",
        "outputId": "b63faf2d-e613-4881-8bc0-12f028c89af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc = np.mean(yhat_tsp == val_pt)\n",
        "print('Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuaracy = 0.641892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqBsRAgYmzWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(val_pt, yhat_tsp, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjRHbOic4Q5v",
        "colab_type": "code",
        "outputId": "0cd7ae54-57ec-4ced-d8df-b6425fd935ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(\"Past:\")\n",
        "unique, counts = np.unique(np.concatenate((tr_pt,val_pt)), return_counts=True)\n",
        "print(\"Total: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(tr_pt, return_counts=True)\n",
        "print(\"Train: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(val_pt, return_counts=True)\n",
        "print(\"Test: \",dict(zip(unique, counts)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Past:\n",
            "Total:  {0.0: 1452, 1.0: 916}\n",
            "Train:  {0.0: 1100, 1.0: 676}\n",
            "Test:  {0.0: 352, 1.0: 240}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1x5L7U3l1jC",
        "colab_type": "code",
        "outputId": "06501693-4b8b-4b65-8b8c-5c0dfeba58e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "testfile=datapath+'test.txt'\n",
        "ts_st,ts_ft,ts_pt=load_test(testfile)\n",
        "\n",
        "#preprocessing\n",
        "ts_st = preprocess_sentences(ts_st)\n",
        "\n",
        "#feature extraction\n",
        "ts_st = sent_to_features(ts_st)\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_bSNz61cG5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ts_st = vectorizer.transform(ts_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWd2VxQ4bmy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts_st = tfidfconverter.transform(ts_st)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LmzFJ32emgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_val_st = vectorizer.transform(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM8QC_ikeneN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_val_st = tfidfconverter.transform(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "287lkbHfmeMS",
        "colab_type": "code",
        "outputId": "1623c476-d592-4484-e95e-99b7c1ee6c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "documents = np.concatenate((tr_st,val_st))\n",
        "print(len(documents))\n",
        "tr_val_st = meanevconverter.transform(documents)\n",
        "ts_st = meanevconverter.transform(ts_st)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ll4LYuczULY",
        "colab_type": "code",
        "outputId": "3bdfa444-035b-4c01-d6ef-367b8ba07c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "\n",
        "svc2 = svm.SVC(probability=True, kernel=\"linear\", C=0.1, gamma=0.001, verbose=10)\n",
        "print(tr_val_st.shape,tr_ft.shape,val_ft.shape)\n",
        "svc2.fit(tr_val_st,np.concatenate((tr_ft,val_ft)))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2368, 300) (1776,) (592,)\n",
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
              "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0jWpS90zX0c",
        "colab_type": "code",
        "outputId": "0d25dde2-44e6-496b-ca57-38a8e2e2b407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yhat_testing = svc2.predict(ts_st)\n",
        "accu = np.mean(yhat_testing == ts_ft)\n",
        "print('Test Accuracy: ',accu)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.5059021922428331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFKwHCsz0TnC",
        "colab_type": "code",
        "outputId": "894ea20e-c03a-4ae0-c8a1-9971ee0f38a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(ts_ft, yhat_testing, target_names=target_names))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.00      0.00      0.00       293\n",
            "     class 1       0.51      1.00      0.67       300\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       593\n",
            "   macro avg       0.25      0.50      0.34       593\n",
            "weighted avg       0.26      0.51      0.34       593\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCsf3tt91ruc",
        "colab_type": "code",
        "outputId": "063c0781-3a11-41b4-e30b-b816a5c88cce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "svc2 = svm.SVC(probability=True, kernel=\"linear\", C=1, gamma=0.001, verbose=10)\n",
        "svc2.fit(tr_val_st,np.concatenate((tr_pt,val_pt)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
              "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI_bjUqr1t6E",
        "colab_type": "code",
        "outputId": "176edde4-19d2-48c0-aab7-0d1e9542d13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yhat_testing = svc2.predict(ts_st)\n",
        "acc = np.mean(yhat_testing == ts_pt)\n",
        "print('Test Accuracy: ',acc)\n",
        "target_names = ['class 0', 'class 1']\n",
        "# print(classification_report(ts_pt, yhat_testing, target_names=target_names))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.5463743676222597\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}