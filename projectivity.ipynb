{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of projectivity baseline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Eu6pel2nWGBl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import linear_model, preprocessing\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from pandas import ExcelWriter\n",
        "from pandas import ExcelFile\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "# nltk.download('stopwords')\n",
        "# nltk.download()\n",
        "from nltk.corpus import stopwords "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5DUffYSxtJl_",
        "colab_type": "code",
        "outputId": "968f155a-1be6-4201-9fd9-fd46244bf7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4116
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('all')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "VBGoLlU_WIi4",
        "colab_type": "code",
        "outputId": "778c0337-0f30-446e-ddfc-0444009e32a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "# data_path='./'\n",
        "# os.chdir(data_path)\n",
        "files=glob.glob('*.txt')\n",
        "\n",
        "sents=[]\n",
        "tagsP=[]\n",
        "tagsF=[]\n",
        "emptyin=[]\n",
        "for f in files:\n",
        "    file = open(f)\n",
        "    df= pd.read_csv(file,delimiter='\\t',encoding='utf-8')\n",
        "    emptyin=df[df['Past'].isnull()].index.tolist()\n",
        "    st=np.delete(np.array(df['sentence'].values),emptyin)\n",
        "    ft=np.delete(np.array(df['Future'].values),emptyin)\n",
        "    pt=np.delete(np.array(df['Past'].values),emptyin)\n",
        "    sents.extend(list(st))\n",
        "    tagsF.extend(list(ft))\n",
        "    tagsP.extend(list(pt))\n",
        "    print(emptyin)\n",
        "X=np.asarray(sents)\n",
        "yf=np.asarray(tagsF)\n",
        "yp=np.asarray(tagsP)\n",
        "print(len(X))\n",
        "print(len(yf))\n",
        "print(len(yp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[103, 266, 280, 301, 314, 392, 412, 474, 664, 685, 688, 778, 875, 926, 975]\n",
            "[49, 82, 195, 258, 533, 541, 577, 648, 726, 942]\n",
            "[51, 90, 214, 273, 304, 404, 458, 511, 527, 640, 664, 769, 911, 995]\n",
            "2961\n",
            "2961\n",
            "2961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8vMJZxPyQ6iF",
        "colab_type": "code",
        "outputId": "1d1f74a3-d0dd-490f-c0ff-aa1123730911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "f='projectivity.txt'\n",
        "file=open(f)\n",
        "df= pd.read_csv(file,delimiter='\\t',encoding='utf-8')\n",
        "emptyin=df[df['Past'].isnull()].index\n",
        "dt=df.loc[:, 'id':'Future']\n",
        "dt=dt.drop(emptyin)\n",
        "print(len(dt))\n",
        "# dt=np.delete(np.array(df.loc[:, 'A':'E'].values),emptyin)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W8l5b-sHCCF2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_data_and_store(df):\n",
        "  train, validation, test = np.split(df, [int(.6*len(df)), int(.8*len(df))])\n",
        "  train.to_csv('train', sep='\\t')\n",
        "  validation.to_csv('validation', sep='\\t')\n",
        "  test.to_csv('test', sep='\\t')\n",
        "  print('Done!')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Utar4lc9alxG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split_data_and_store(dt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OhIpVrntXle-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_train_val(train,val):\n",
        "  df= pd.read_csv(train,delimiter='\\t',encoding='utf-8')\n",
        "  tr_st=np.array(df['sentence'].values)\n",
        "  tr_ft=np.array(df['Future'].values)\n",
        "  tr_pt=np.array(df['Past'].values)\n",
        "  df= pd.read_csv(val,delimiter='\\t',encoding='utf-8')\n",
        "  val_st=np.array(df['sentence'].values)\n",
        "  val_ft=np.array(df['Future'].values)\n",
        "  val_pt=np.array(df['Past'].values)\n",
        "  return tr_st,tr_ft,tr_pt, val_st,val_ft,val_pt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FxxZx8cfjN4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_test(test):\n",
        "  df= pd.read_csv(test,delimiter='\\t',encoding='utf-8')\n",
        "  ts_st=np.array(df['sentence'].values)\n",
        "  ts_ft=np.array(df['Future'].values)\n",
        "  ts_pt=np.array(df['Past'].values)\n",
        "  return ts_st,ts_ft,ts_pt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "olOQd2m9VDBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_sentences(X):\n",
        "  documents = []\n",
        "  stemmer = WordNetLemmatizer()\n",
        "  for sen in range(0, len(X)):\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    document = re.sub(r'^\"', '', document)\n",
        "    document = re.sub(r'\"$', '', document)\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    document = document.lower()\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    #     document = ' '.join(document)\n",
        "    documents.append(document)\n",
        "  print(len(documents))\n",
        "  return documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zQlVvKEebN01",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sent_to_features(documents):\n",
        "  feature_docs=[]\n",
        "  for document in documents:\n",
        "    tokens=nltk.word_tokenize(document)\n",
        "    bigrams = list(nltk.bigrams(tokens))\n",
        "    #pos_bigrams = list(nltk.bigrams([pos for (word,pos) in nltk.pos_tag(tokens)]))\n",
        "    document=[word+' '+pos for (word,pos) in nltk.pos_tag(tokens)]\n",
        "    document.extend([w1+'_'+w2 for (w1,w2) in bigrams])\n",
        "    #     document.extend([w1+'_'+w2 for (w1,w2) in pos_bigrams])\n",
        "    #     document.extend([pos for (word,pos) in nltk.pos_tag(tokens)])\n",
        "    #     print(document)\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    feature_docs.append(document)\n",
        "  return feature_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7DHUdtgJXlKG",
        "colab_type": "code",
        "outputId": "5c6b679b-0091-4818-98ed-08497ecc8afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# documents = []\n",
        "# stemmer = WordNetLemmatizer()\n",
        "# for sen in range(0, len(X)):\n",
        "#   document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "#   document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "#   document = re.sub(r'^\"', '', document)\n",
        "#   document = re.sub(r'\"$', '', document)\n",
        "#   document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "#   document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "#   document = re.sub(r'^b\\s+', '', document)\n",
        "#   document = document.lower()\n",
        "#   #     document = document.split()\n",
        "#   #     document = [stemmer.lemmatize(word) for word in document]\n",
        "#   #     document = ' '.join(document)\n",
        "#   documents.append(document)\n",
        "# print(len(documents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2VjfiO4FeJz6",
        "colab_type": "code",
        "outputId": "924467cd-3f7e-49a7-de6d-9824be8adbdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "#loading train and validation data\n",
        "trainfile='train.txt'\n",
        "valfile='validation.txt'\n",
        "tr_st,tr_ft,tr_pt, val_st,val_ft,val_pt=load_train_val(trainfile,valfile)\n",
        "\n",
        "#preprocessing\n",
        "tr_st = preprocess_sentences(tr_st)\n",
        "val_st = preprocess_sentences(val_st)\n",
        "\n",
        "#feature extraction\n",
        "tr_st = sent_to_features(tr_st)\n",
        "val_st = sent_to_features(val_st)\n",
        "\n",
        "print('Train size: ',len(tr_st))\n",
        "print('Val size: ',len(val_st))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1776\n",
            "592\n",
            "Train size:  1776\n",
            "Val size:  592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bUwpVDvpCT7n",
        "colab_type": "code",
        "outputId": "5fb4ea9f-01f1-45c1-c24e-a5d48653c4d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(tr_st[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "friends NNS and CC adversaries NNS abroad RB were VBD asking VBG whether IN america NN had VBD lost VBN its PRP$ nerve NN friends_and and_adversaries adversaries_abroad abroad_were were_asking asking_whether whether_america america_had had_lost lost_its its_nerve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kZoEsHeU9xbq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMO_e_OTXn3u",
        "colab_type": "code",
        "outputId": "042ddba8-141f-4d9b-d7dd-1c8d1f234c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  \n",
        "vectorizer = CountVectorizer()  #, stop_words=stopwords.words('english')\n",
        "documents = np.concatenate((tr_st,val_st))\n",
        "print(len(documents))\n",
        "vectorizer.fit(documents)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "        tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "metadata": {
        "id": "en7u9Sbpq3Lg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tr_st = vectorizer.transform(tr_st)\n",
        "val_st = vectorizer.transform(val_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2Ugxes-Xp_W",
        "colab_type": "code",
        "outputId": "d01853ea-bcf8-434e-bda4-7d1982631bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(tr_st.shape)\n",
        "# print(X[0][:1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1776, 39471)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K8t3ksPKaWWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f113c9ad-bc63-43db-f7c6-0c0deb9bf5c9"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "tfidfconverter = TfidfVectorizer()#min_df=2, max_df=0.8, stop_words=stopwords.words('english')\n",
        "documents = np.concatenate((tr_st,val_st))\n",
        "print(len(documents))\n",
        "tfidfconverter.fit(documents) \n",
        "tr_st = tfidfconverter.transform(tr_st)\n",
        "val_st = tfidfconverter.transform(val_st)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d2RoOKcNaZQr",
        "colab_type": "code",
        "outputId": "506a0089-3dd9-4ab1-cb95-23230dcbb3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(tr_st.shape)\n",
        "# print(Xt[0][:1000])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1776, 39471)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F8QRMPpBkIYC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split  \n",
        "# Xtr, Xts, ytr, yts = train_test_split(X, yf, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sOqLdeuWj4MF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "svc = svm.SVC(probability=True, kernel=\"linear\", C=2.8, gamma=.0073,verbose=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1fSdNcx6j5LZ",
        "colab_type": "code",
        "outputId": "e9f21400-4af9-4ae4-9ce2-55c173024ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "svc.fit(tr_st,tr_ft)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=2.8, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.0073, kernel='linear',\n",
              "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "VGeCYbYnlLby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yhat_ts = svc.predict(val_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xCpFiToN-mjW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yhat_tr = svc.predict(Xtr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LWLVkclj-sF2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc = np.mean(yhat_tr == ytr)\n",
        "print('Training Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVJLNXdqj7eB",
        "colab_type": "code",
        "outputId": "1c4dedcb-170e-40b8-fd36-9e79cd4a3e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "acc = np.mean(yhat_ts == val_ft)\n",
        "print('Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuaracy = 0.790541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gWMtkPSHnGcu",
        "colab_type": "code",
        "outputId": "82ad95c0-ccba-4ea6-ad5f-ccb457e4320f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(val_ft, yhat_ts, target_names=target_names))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.76      0.75      0.75       256\n",
            "     class 1       0.81      0.82      0.82       336\n",
            "\n",
            "   micro avg       0.79      0.79      0.79       592\n",
            "   macro avg       0.79      0.79      0.79       592\n",
            "weighted avg       0.79      0.79      0.79       592\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5ZLnofLy3K_g",
        "colab_type": "code",
        "outputId": "111dfd69-eeb7-4ff1-dfda-2b9d3bb29073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Future:\")\n",
        "unique, counts = np.unique(np.concatenate((tr_ft,val_ft)), return_counts=True)\n",
        "print(\"Total: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(tr_ft, return_counts=True)\n",
        "print(\"Train: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(val_ft, return_counts=True)\n",
        "print(\"Test: \",dict(zip(unique, counts)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Future:\n",
            "Total:  {0.0: 1012, 1.0: 1356}\n",
            "Train:  {0.0: 756, 1.0: 1020}\n",
            "Test:  {0.0: 256, 1.0: 336}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6N7kXxvjvWZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Xtr, Xts, ytrp, ytsp = train_test_split(X, yp, test_size=0.3, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s8gI3gWYkJ5q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svcp = svm.SVC(probability=True, kernel=\"linear\", C=2.8, gamma=.0073,verbose=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ORc9bdqhviuc",
        "colab_type": "code",
        "outputId": "7eb5a79c-eecb-49e4-d48f-9e9211f062e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "svcp.fit(tr_st,tr_pt)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=2.8, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.0073, kernel='linear',\n",
              "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "8td74YeCvu7X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yhat_tsp = svcp.predict(val_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HHUgtXX8-4m2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yhat_trp = svcp.predict(Xtr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WTFb9QVr-7nb",
        "colab_type": "code",
        "outputId": "ab12dad1-e0da-4b61-b8b8-192204fb1574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "acc = np.mean(yhat_trp == ytrp)\n",
        "print('Training Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuaracy = 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "anOCYdy4vzsg",
        "colab_type": "code",
        "outputId": "f5bdf3c9-d210-4d49-9818-cec987f1c04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "acc = np.mean(yhat_tsp == val_pt)\n",
        "print('Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuaracy = 0.792230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dqBsRAgYmzWc",
        "colab_type": "code",
        "outputId": "8198b592-7bc1-4fe7-c4e0-45a684146602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "cell_type": "code",
      "source": [
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(val_pt, yhat_tsp, target_names=target_names))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.78      0.90      0.84       352\n",
            "     class 1       0.81      0.63      0.71       240\n",
            "\n",
            "   micro avg       0.79      0.79      0.79       592\n",
            "   macro avg       0.80      0.77      0.77       592\n",
            "weighted avg       0.79      0.79      0.79       592\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zjRHbOic4Q5v",
        "colab_type": "code",
        "outputId": "0cd7ae54-57ec-4ced-d8df-b6425fd935ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Past:\")\n",
        "unique, counts = np.unique(np.concatenate((tr_pt,val_pt)), return_counts=True)\n",
        "print(\"Total: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(tr_pt, return_counts=True)\n",
        "print(\"Train: \",dict(zip(unique, counts)))\n",
        "unique, counts = np.unique(val_pt, return_counts=True)\n",
        "print(\"Test: \",dict(zip(unique, counts)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Past:\n",
            "Total:  {0.0: 1452, 1.0: 916}\n",
            "Train:  {0.0: 1100, 1.0: 676}\n",
            "Test:  {0.0: 352, 1.0: 240}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xPIWXyWpwEIZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "C_test = [0.1,1,2.5,5,10]\n",
        "gam_test = [0.001,0.01,0.1]\n",
        "\n",
        "nC = len(C_test)\n",
        "ngam = len(gam_test)\n",
        "acc = np.zeros((nC,ngam))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q-zgi74iwKz7",
        "colab_type": "code",
        "outputId": "97ec4098-2bc9-4535-fbc4-c277d3a3296f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "cell_type": "code",
      "source": [
        "# Xtr, Xts, ytr, yts = train_test_split(X, yf, test_size=0.3, random_state=42)\n",
        "\n",
        "# TODO:  Print the accuracy matrix\n",
        "for index_C , C in enumerate(C_test):\n",
        "  for index_G , gamma in enumerate(gam_test):\n",
        "    svc2 = svm.SVC(probability=True, kernel=\"linear\", C=C, gamma=gamma, verbose=10)\n",
        "    svc2.fit(tr_st,tr_ft)\n",
        "    yhat_testing_cross = svc2.predict(val_st)\n",
        "    acc[index_C , index_G ] = np.mean(yhat_testing_cross == val_ft)\n",
        "    print(C,gamma)\n",
        "    print('Acc', acc[index_C,index_G])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]0.1 0.001\n",
            "Acc 0.7922297297297297\n",
            "[LibSVM]0.1 0.01\n",
            "Acc 0.7922297297297297\n",
            "[LibSVM]0.1 0.1\n",
            "Acc 0.7922297297297297\n",
            "[LibSVM]1 0.001\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]1 0.01\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]1 0.1\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]2.5 0.001\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]2.5 0.01\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]2.5 0.1\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]5 0.001\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]5 0.01\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]5 0.1\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]10 0.001\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]10 0.01\n",
            "Acc 0.7719594594594594\n",
            "[LibSVM]10 0.1\n",
            "Acc 0.7719594594594594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zyYGDe8xwnvJ",
        "colab_type": "code",
        "outputId": "0a77f3e2-49ad-4f9f-dcc4-ef73b2105311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "optimal_index_C , optimal_index_Gamma = np.unravel_index(acc.argmax(),acc.shape)\n",
        "\n",
        "print(\"Maximum Accuracy\",acc[optimal_index_C][optimal_index_Gamma])\n",
        "opt_c_f = C_test[optimal_index_C]\n",
        "opt_g_f = gam_test[optimal_index_Gamma]\n",
        "print(\"Optimal C:\", C_test[optimal_index_C])\n",
        "print('Optimal Gamma:' , gam_test[optimal_index_Gamma])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum Accuracy 0.7922297297297297\n",
            "Optimal C: 0.1\n",
            "Optimal Gamma: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T1x5L7U3l1jC",
        "colab_type": "code",
        "outputId": "d89ad08e-feb1-41b8-8742-75d0db5a7f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "testfile='test.txt'\n",
        "ts_st,ts_ft,ts_pt=load_test(testfile)\n",
        "\n",
        "#preprocessing\n",
        "ts_st = preprocess_sentences(ts_st)\n",
        "\n",
        "#feature extraction\n",
        "ts_st = sent_to_features(ts_st)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a_bSNz61cG5k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "ts_st = vectorizer.transform(ts_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BWd2VxQ4bmy2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ts_st = tfidfconverter.transform(ts_st)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LmzFJ32emgK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tr_val_st = vectorizer.transform(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xM8QC_ikeneN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tr_val_st = tfidfconverter.transform(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Ll4LYuczULY",
        "colab_type": "code",
        "outputId": "3cc0b93c-e262-4f16-fc11-92a1581bcd43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "svc2 = svm.SVC(probability=True, kernel=\"linear\", C=0.1, gamma=0.001, verbose=10)\n",
        "print(tr_val_st.shape,tr_ft.shape,val_ft.shape)\n",
        "svc2.fit(tr_val_st,np.concatenate((tr_ft,val_ft)))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2368, 39471) (1776,) (592,)\n",
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
              "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "C0jWpS90zX0c",
        "colab_type": "code",
        "outputId": "de9772e9-feb4-49c2-f2b3-7c47239ce9e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "yhat_testing = svc2.predict(ts_st)\n",
        "accu = np.mean(yhat_testing == ts_ft)\n",
        "print('Test Accuracy: ',accu)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.5598650927487352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dFKwHCsz0TnC",
        "colab_type": "code",
        "outputId": "9853163c-fdce-4970-ac4a-911aa0ff0a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "cell_type": "code",
      "source": [
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(ts_ft, yhat_testing, target_names=target_names))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.97      0.11      0.20       293\n",
            "     class 1       0.53      1.00      0.70       300\n",
            "\n",
            "   micro avg       0.56      0.56      0.56       593\n",
            "   macro avg       0.75      0.55      0.45       593\n",
            "weighted avg       0.75      0.56      0.45       593\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6DgT0Dvjxj6H",
        "colab_type": "code",
        "outputId": "2961c4f3-e59f-42a2-d8dd-7f57b354eb0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "cell_type": "code",
      "source": [
        "# Xtr, Xts, ytrp, ytsp = train_test_split(X, yp, test_size=0.3, random_state=42)\n",
        "C_test = [0.1,1,2.5,5,10]\n",
        "gam_test = [0.001,0.01,0.1]\n",
        "\n",
        "nC = len(C_test)\n",
        "ngam = len(gam_test)\n",
        "acc = np.zeros((nC,ngam))\n",
        "# TODO:  Print the accuracy matrix\n",
        "for index_C , C in enumerate(C_test):\n",
        "  for index_G , gamma in enumerate(gam_test):\n",
        "    svc2 = svm.SVC(probability=True, kernel=\"linear\", C=C, gamma=gamma, verbose=10)\n",
        "    svc2.fit(tr_st,tr_pt)\n",
        "    yhat_testing_cross = svc2.predict(val_st)\n",
        "    acc[index_C , index_G] = np.mean(yhat_testing_cross == val_pt)\n",
        "    print(C,gamma)\n",
        "    print('Acc', acc[index_C,index_G])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]0.1 0.001\n",
            "Acc 0.7871621621621622\n",
            "[LibSVM]0.1 0.01\n",
            "Acc 0.7871621621621622\n",
            "[LibSVM]0.1 0.1\n",
            "Acc 0.7871621621621622\n",
            "[LibSVM]1 0.001\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]1 0.01\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]1 0.1\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]2.5 0.001\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]2.5 0.01\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]2.5 0.1\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]5 0.001\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]5 0.01\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]5 0.1\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]10 0.001\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]10 0.01\n",
            "Acc 0.785472972972973\n",
            "[LibSVM]10 0.1\n",
            "Acc 0.785472972972973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vPI36usTyyU4",
        "colab_type": "code",
        "outputId": "9e39d1d4-14b4-43e4-e9e7-99bcadc0589f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "optimal_index_C , optimal_index_Gamma = np.unravel_index(acc.argmax(),acc.shape)\n",
        "\n",
        "print(\"Maximum Accuracy\",acc[optimal_index_C][optimal_index_Gamma])\n",
        "opt_c_p = C_test[optimal_index_C]\n",
        "opt_g_p = gam_test[optimal_index_Gamma]\n",
        "print(\"Optimal C:\", C_test[optimal_index_C])\n",
        "print('Optimal Gamma:' , gam_test[optimal_index_Gamma])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum Accuracy 0.7871621621621622\n",
            "Optimal C: 0.1\n",
            "Optimal Gamma: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lCsf3tt91ruc",
        "colab_type": "code",
        "outputId": "6ead52cc-ed5c-4c48-aeac-b0207e543f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "svc2 = svm.SVC(probability=True, kernel=\"linear\", C=0.1, gamma=0.001, verbose=10)\n",
        "svc2.fit(tr_val_st,np.concatenate((tr_pt,val_pt)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
              "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "EI_bjUqr1t6E",
        "colab_type": "code",
        "outputId": "80427fc9-7ac2-4667-d167-598916de1850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "yhat_testing = svc2.predict(ts_st)\n",
        "acc = np.mean(yhat_testing == ts_pt)\n",
        "print('Test Accuracy: ',acc)\n",
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(ts_pt, yhat_testing, target_names=target_names))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.6003372681281619\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.58      1.00      0.73       324\n",
            "     class 1       0.97      0.12      0.22       269\n",
            "\n",
            "   micro avg       0.60      0.60      0.60       593\n",
            "   macro avg       0.77      0.56      0.47       593\n",
            "weighted avg       0.76      0.60      0.50       593\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}