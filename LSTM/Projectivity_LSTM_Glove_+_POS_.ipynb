{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projectivity LSTM Glove + POS .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9GM_TH5roB0",
        "colab_type": "text"
      },
      "source": [
        "Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu6pel2nWGBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import linear_model, preprocessing\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "# nltk.download('stopwords')\n",
        "# nltk.download()\n",
        "from nltk.corpus import stopwords "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DUffYSxtJl_",
        "colab_type": "code",
        "outputId": "7ead7b9f-e469-41fb-adb6-c61acfb97f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4486
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('all')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxZrfr29nAm3",
        "colab_type": "text"
      },
      "source": [
        "Compile and run from here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIVbzDgMnIHH",
        "colab_type": "text"
      },
      "source": [
        "To load the train and validation data from TSV files.\n",
        "\n",
        "Arguments: train file, validation file\n",
        "\n",
        "Returns:\n",
        "\n",
        "tr_st: Training sentences\n",
        "tr_ft: Training Future Labels\n",
        "tr_pt: Trainiing past labels\n",
        "val_st: Validation sentences\n",
        "val_ft: Validation Future Labels\n",
        "val_pt: Validation past labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhIpVrntXle-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_train_val(train,val):\n",
        "  df= pd.read_csv(train,delimiter='\\t',encoding='utf-8')\n",
        "  tr_st=np.array(df['sentence'].values)\n",
        "  tr_ft=np.array(df['Future'].values)\n",
        "  tr_pt=np.array(df['Past'].values)\n",
        "  df= pd.read_csv(val,delimiter='\\t',encoding='utf-8')\n",
        "  val_st=np.array(df['sentence'].values)\n",
        "  val_ft=np.array(df['Future'].values)\n",
        "  val_pt=np.array(df['Past'].values)\n",
        "  return tr_st,tr_ft,tr_pt, val_st,val_ft,val_pt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZFGdfLUm_-d",
        "colab_type": "text"
      },
      "source": [
        "Method to load the test data from TSV file.\n",
        "\n",
        "Arguments: test file\n",
        "\n",
        "Returns:\n",
        "\n",
        "ts_st: Test sentences\n",
        "ts_ft: Test Future Labels\n",
        "ts_pt: Test past labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxxZx8cfjN4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_test(test):\n",
        "  df= pd.read_csv(test,delimiter='\\t',encoding='utf-8')\n",
        "  ts_st=np.array(df['sentence'].values)\n",
        "  ts_ft=np.array(df['Future'].values)\n",
        "  ts_pt=np.array(df['Past'].values)\n",
        "  return ts_st,ts_ft,ts_pt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z8FXCuHoRNM",
        "colab_type": "text"
      },
      "source": [
        "To preprocess sentences:\n",
        "Removing some special characters, spaces and tabs, and conversion to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olOQd2m9VDBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentences(X):\n",
        "  documents = []\n",
        "  stemmer = WordNetLemmatizer()\n",
        "  for sen in range(0, len(X)):\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    document = re.sub(r'^\"', '', document)\n",
        "    document = re.sub(r'\"$', '', document)\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    document = document.lower()\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    #     document = ' '.join(document)\n",
        "    documents.append(document)\n",
        "  print(len(documents))\n",
        "  return documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUjEdlLqoju2",
        "colab_type": "text"
      },
      "source": [
        "Converts sentences to lists of features\n",
        "\n",
        "Uncomment code as needed for features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQlVvKEebN01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_features(documents):\n",
        "  feature_docs=[]\n",
        "  for document in documents:\n",
        "    tokens=nltk.word_tokenize(document)\n",
        "    bigrams = list(nltk.bigrams(tokens))\n",
        "    #pos_bigrams = list(nltk.bigrams([pos for (word,pos) in nltk.pos_tag(tokens)]))\n",
        "    document=[word+' '+pos for (word,pos) in nltk.pos_tag(tokens)]#+' '+pos\n",
        "#     document.extend([w1+'_'+w2 for (w1,w2) in bigrams])\n",
        "    #     document.extend([w1+'_'+w2 for (w1,w2) in pos_bigrams])\n",
        "    #     document.extend([pos for (word,pos) in nltk.pos_tag(tokens)])\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    feature_docs.append(document)\n",
        "  return feature_docs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iQzX5Oyo6hl",
        "colab_type": "text"
      },
      "source": [
        "This is not necesasry unless the code is being run on a Colab notebook, and the data is stored on Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYP9853kPwtg",
        "colab_type": "code",
        "outputId": "9575b246-acfa-4c27-a1f5-bfcece22a502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmgo2XqVpNL1",
        "colab_type": "text"
      },
      "source": [
        "Path to the location where the data resides"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKhqgC5FQHyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapath = 'gdrive/My Drive/data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "westpD8MpWDa",
        "colab_type": "text"
      },
      "source": [
        "Loading the train and validation data\n",
        "\n",
        "Preprocessing the data\n",
        "\n",
        "Feature extraction if necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjfiO4FeJz6",
        "colab_type": "code",
        "outputId": "2e597a18-73c3-4b43-ed7d-445e47aba8a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#loading train and validation data\n",
        "trainfile=datapath+'train.txt'\n",
        "valfile=datapath+'validation.txt'\n",
        "tr_st,tr_ft,tr_pt, val_st,val_ft,val_pt=load_train_val(trainfile,valfile)\n",
        "\n",
        "#preprocessing\n",
        "tr_st = preprocess_sentences(tr_st)\n",
        "val_st = preprocess_sentences(val_st)\n",
        "\n",
        "#feature extraction\n",
        "tr_st = sent_to_features(tr_st)\n",
        "val_st = sent_to_features(val_st)\n",
        "\n",
        "print('Train size: ',len(tr_st))\n",
        "print('Val size: ',len(val_st))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1776\n",
            "592\n",
            "Train size:  1776\n",
            "Val size:  592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUwpVDvpCT7n",
        "colab_type": "code",
        "outputId": "5d6355bb-4e70-4b73-9a4e-ea61307e093f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tr_st[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "friends NNS and CC adversaries NNS abroad RB were VBD asking VBG whether IN america NN had VBD lost VBN its PRP$ nerve NN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhyHmoYhpl5V",
        "colab_type": "text"
      },
      "source": [
        "Imports necessary for LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VngaIqK5HAoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, CuDNNLSTM, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import optimizers\n",
        "\n",
        "# from tensorflow.keras.backend import set_session\n",
        "import keras.backend as K\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2DL356Ypscx",
        "colab_type": "text"
      },
      "source": [
        "Creating a Keras Tokenizer and fitting it on train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LFo1Iwafnvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(np.concatenate((tr_st,val_st)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV3xBHu2se9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "pos_model = Word2Vec.load(datapath+'model_300d.bin')\n",
        "# print(pos_model)\n",
        "words = list(pos_model.wv.vocab)\n",
        "# print(words)\n",
        "# print(len(words))\n",
        "# print(pos_model['NN'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrBOaXLzqAsA",
        "colab_type": "text"
      },
      "source": [
        "Initializing vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flu9dPuMfuuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size = len(tokenizer.word_index)+1\n",
        "# print(tokenizer.word_counts)\n",
        "# print(tokenizer.document_count)\n",
        "# print(tokenizer.word_index)\n",
        "# print(tokenizer.word_docs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh-cLMCvihEX",
        "colab_type": "code",
        "outputId": "2d0977e7-adc2-4fcb-b201-0527b2c9b124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(vocabulary_size)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_H4rwCgqT9Y",
        "colab_type": "text"
      },
      "source": [
        "Extracting sequences from train and validation data, and padding them to a maximum length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrQ-RHoOG7Jf",
        "colab_type": "code",
        "outputId": "4fffc0f8-61bd-4191-cdf3-dad878fd6bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "# vocabulary_size = 7000\n",
        "# tokenizer.fit_on_texts(np.concatenate((tr_st,val_st)))\n",
        "MAX_LEN=145+145\n",
        "sequences = tokenizer.texts_to_sequences(tr_st)\n",
        "tr_data = pad_sequences(sequences, maxlen=MAX_LEN)\n",
        "sequences = tokenizer.texts_to_sequences(val_st)\n",
        "val_data = pad_sequences(sequences, maxlen=MAX_LEN)\n",
        "\n",
        "print(len(tr_data))\n",
        "print(len(val_data))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1776\n",
            "592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaPP6ZFFqf05",
        "colab_type": "text"
      },
      "source": [
        "Loading Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGKoxGURzgxb",
        "colab_type": "code",
        "outputId": "fb15264f-4801-416e-e589-3c55d12aadcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f = open(datapath+'glove.6B.300d.txt')\n",
        "embedding_values = {}\n",
        "for line in tqdm(f):\n",
        "  value = line.split(' ')\n",
        "  word = value[0]\n",
        "  coef = np.array(value[1:],dtype = 'float32')\n",
        "  embedding_values[word]=coef"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000it [00:37, 10784.79it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFY6sYIJtfxd",
        "colab_type": "text"
      },
      "source": [
        "Adding embedding values of POS tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y121AORfjwmJ",
        "colab_type": "code",
        "outputId": "28ec256b-f348-4f33-ffdf-32ceb249768f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "for word in words:\n",
        "  coef=np.array(pos_model[word],dtype = 'float32')\n",
        "  embedding_values[word]=coef"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4naBxGnFuoCh",
        "colab_type": "code",
        "outputId": "a54d2d40-0536-4b02-8bfc-27207a9c936a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(embedding_values)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400032"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNBfnm5eqmLe",
        "colab_type": "text"
      },
      "source": [
        "Creating an embedding matrix for words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu234t4CzsfJ",
        "colab_type": "code",
        "outputId": "f81f3dba-25e8-444a-f533-61450e5f5d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_matrix = np.zeros((vocabulary_size,300))\n",
        "for word,i in tqdm(tokenizer.word_index.items()):\n",
        "  values = embedding_values.get(word)\n",
        "  if values is not None:\n",
        "    embedding_matrix[i] = values"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6770/6770 [00:00<00:00, 269536.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Y21Y9aq5ig",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8vIKkokx2Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "from numpy.random import seed\n",
        "from tensorflow import set_random_seed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRHaXxzKcY_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def session_refresh():\n",
        "  seed(2)\n",
        "  set_random_seed(2)\n",
        "  rn.seed(2)\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "  print('refreshed session')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bBpZbINq9If",
        "colab_type": "text"
      },
      "source": [
        "BiLSTM for Future task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoDUaHXHrD8K",
        "colab_type": "text"
      },
      "source": [
        "Stacked BiLSTM for Future task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIAM23tnQ2_m",
        "colab_type": "code",
        "outputId": "5e030fc8-d1e0-41e0-e789-844ee2514666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "#Future LSTM2\n",
        "# K.clear_session()\n",
        "K.set_session(sess)\n",
        "model2 = Sequential()\n",
        "print(MAX_LEN)\n",
        "model2.add(Embedding(vocabulary_size,300, input_length = MAX_LEN,weights = [embedding_matrix],trainable = False))\n",
        "# model.add(Embedding(vocabulary_size, 145, input_length=145))\n",
        "model2.add(Bidirectional(LSTM(145, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
        "model2.add(Bidirectional(LSTM(145, dropout=0.1, recurrent_dropout=0.1)))\n",
        "model2.add(Dense(64,activation = 'relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "\n",
        "filepath=datapath+'/models/'+\"future-2-pos-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])\n",
        "# model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model2.summary()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "290\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 290, 300)          2031300   \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 290, 290)          517360    \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 290)               505760    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                18624     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,073,109\n",
            "Trainable params: 1,041,809\n",
            "Non-trainable params: 2,031,300\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJT0QXx3SI1C",
        "colab_type": "code",
        "outputId": "3b0a00ff-d17e-4b26-bae4-519e64d2e54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "model2.fit(tr_data, tr_ft, validation_data=(val_data,val_ft), epochs=10,callbacks=callbacks_list,batch_size=64)  #epoch 15 batch 128 no relu hidden   epoch 7 batch 128 w/ relu hidden\n",
        "model2_json = model2.to_json()\n",
        "with open(datapath+\"future-2-pos-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model2_json)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 1776 samples, validate on 592 samples\n",
            "Epoch 1/10\n",
            "1776/1776 [==============================] - 54s 31ms/step - loss: 0.7051 - acc: 0.5755 - val_loss: 1.1178 - val_acc: 0.4324\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.43243, saving model to gdrive/My Drive/data//models/future-2-pos-best.hdf5\n",
            "Epoch 2/10\n",
            "1776/1776 [==============================] - 51s 29ms/step - loss: 0.6604 - acc: 0.6875 - val_loss: 0.5319 - val_acc: 0.7584\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.43243 to 0.75845, saving model to gdrive/My Drive/data//models/future-2-pos-best.hdf5\n",
            "Epoch 3/10\n",
            "1776/1776 [==============================] - 51s 29ms/step - loss: 0.5335 - acc: 0.7573 - val_loss: 0.5221 - val_acc: 0.7652\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.75845 to 0.76520, saving model to gdrive/My Drive/data//models/future-2-pos-best.hdf5\n",
            "Epoch 4/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.5003 - acc: 0.7652 - val_loss: 0.5096 - val_acc: 0.7855\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.76520 to 0.78547, saving model to gdrive/My Drive/data//models/future-2-pos-best.hdf5\n",
            "Epoch 5/10\n",
            "1776/1776 [==============================] - 52s 29ms/step - loss: 0.4735 - acc: 0.7793 - val_loss: 0.5029 - val_acc: 0.7432\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.78547\n",
            "Epoch 6/10\n",
            "1776/1776 [==============================] - 53s 30ms/step - loss: 0.4627 - acc: 0.7945 - val_loss: 0.4469 - val_acc: 0.8007\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.78547 to 0.80068, saving model to gdrive/My Drive/data//models/future-2-pos-best.hdf5\n",
            "Epoch 7/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.4379 - acc: 0.8131 - val_loss: 0.4852 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.80068\n",
            "Epoch 8/10\n",
            "1776/1776 [==============================] - 51s 29ms/step - loss: 0.4171 - acc: 0.8125 - val_loss: 0.4547 - val_acc: 0.7838\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.80068\n",
            "Epoch 9/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.3979 - acc: 0.8260 - val_loss: 0.4536 - val_acc: 0.8057\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.80068 to 0.80574, saving model to gdrive/My Drive/data//models/future-2-pos-best.hdf5\n",
            "Epoch 10/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.4066 - acc: 0.8249 - val_loss: 0.4626 - val_acc: 0.7872\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.80574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-23eb1e4410b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_ft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#epoch 15 batch 128 no relu hidden   epoch 7 batch 128 w/ relu hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"future-2-pos-model.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZn2LYYSQK-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR8wePLRrNg7",
        "colab_type": "text"
      },
      "source": [
        "Training on whole training data after parameter tuning, in order to use the model to evaluate the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-IS4iu57ltx",
        "colab_type": "code",
        "outputId": "e160e819-6f0d-4c1b-a6e6-72c92b240b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# model2.fit(np.concatenate((tr_data,val_data)), np.concatenate((tr_ft,val_ft)), epochs=9,batch_size=64)\n",
        "\n",
        "# model_json = model2.to_json()\n",
        "# with open(datapath+'/models/'+\"future-2-full-pos-model.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "# model2.save_weights(datapath+'/models/'+\"future-2-full-pos-best.hdf5\")\n",
        "# print(\"Saved model to disk\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "2368/2368 [==============================] - 62s 26ms/step - loss: 0.6540 - acc: 0.6423\n",
            "Epoch 2/9\n",
            "2368/2368 [==============================] - 60s 25ms/step - loss: 0.5397 - acc: 0.7580\n",
            "Epoch 3/9\n",
            "2368/2368 [==============================] - 60s 25ms/step - loss: 0.5438 - acc: 0.7416\n",
            "Epoch 4/9\n",
            "2368/2368 [==============================] - 61s 26ms/step - loss: 0.4909 - acc: 0.7821\n",
            "Epoch 5/9\n",
            "2368/2368 [==============================] - 59s 25ms/step - loss: 0.4750 - acc: 0.7927\n",
            "Epoch 6/9\n",
            "2368/2368 [==============================] - 60s 25ms/step - loss: 0.4659 - acc: 0.7893\n",
            "Epoch 7/9\n",
            "2368/2368 [==============================] - 60s 25ms/step - loss: 0.4415 - acc: 0.8015\n",
            "Epoch 8/9\n",
            "2368/2368 [==============================] - 59s 25ms/step - loss: 0.4210 - acc: 0.8155\n",
            "Epoch 9/9\n",
            "2368/2368 [==============================] - 61s 26ms/step - loss: 0.4034 - acc: 0.8298\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZWGyo6Gt_Vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI9HnP6YriTJ",
        "colab_type": "text"
      },
      "source": [
        "Loading the Test data\n",
        "\n",
        "preprocessing the data\n",
        "\n",
        "Extracting sequences\n",
        "\n",
        "Padding the sequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om3T_CwSMj_R",
        "colab_type": "code",
        "outputId": "08798f72-0992-49e5-b472-5a047f85ea2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "testfile=datapath+'test.txt'\n",
        "ts_st,ts_ft,ts_pt=load_test(testfile)\n",
        "\n",
        "#preprocessing\n",
        "ts_st = preprocess_sentences(ts_st)\n",
        "\n",
        "#feature extraction\n",
        "#ts_st = sent_to_features(ts_st)\n",
        "sequences = tokenizer.texts_to_sequences(ts_st)\n",
        "ts_data = pad_sequences(sequences, maxlen=MAX_LEN)\n",
        "print(ts_data.shape)\n",
        "print(ts_ft.shape)\n",
        "# test_loss, test_acc = model.evaluate(np.append((ts_data,ts_ft),1))\n",
        "# print('Test Loss: {}'.format(test_loss))\n",
        "# print('Test Accuracy: {}'.format(test_acc))\n",
        "\n",
        "\n",
        "# print('Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "593\n",
            "(593, 290)\n",
            "(593,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ0j2FCCLkkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bec917e2-04dd-4c48-ac9d-750557e28c4b"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "# load json and create model\n",
        "json_file = open(datapath+\"future-2-pos-model.json\", 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model2 = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "model2.load_weights(datapath+'/models/'+\"future-2-pos-best.hdf5\")\n",
        "print(\"Loaded model from disk\")\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFXas8tJscK5",
        "colab_type": "text"
      },
      "source": [
        "Evaluating the test data for future task "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvgVJ-NcsbRC",
        "colab_type": "code",
        "outputId": "65c3f01d-cc55-4bfa-e37a-9c96d84bdabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "scores = model2.evaluate(ts_data, ts_ft, verbose=0)\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 76.22%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69CS2ezdWyoy",
        "colab_type": "text"
      },
      "source": [
        "Future Test Accuracy: 82.80% ; epochs 5+3, model: BiLSTM + Dense\n",
        "\n",
        "Accuracy: 72.68%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW3UEFHSsjeE",
        "colab_type": "text"
      },
      "source": [
        "Generating a classification report for the test data for future task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnKBdfMv6ezo",
        "colab_type": "code",
        "outputId": "2f978c89-8f5e-49a3-ca32-207f9c1bad43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred = model2.predict(ts_data, batch_size=64, verbose=1)\n",
        "y_pred =(y_pred>0.5)\n",
        "\n",
        "print(classification_report(ts_ft, y_pred))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "593/593 [==============================] - 7s 11ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.74      0.75       293\n",
            "         1.0       0.76      0.78      0.77       300\n",
            "\n",
            "   micro avg       0.76      0.76      0.76       593\n",
            "   macro avg       0.76      0.76      0.76       593\n",
            "weighted avg       0.76      0.76      0.76       593\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5H5yWnhXf7l",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Future Test Accuracy: 82.80% ; epochs 5+3, model: BiLSTM + Dense\n",
        "\n",
        "precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.87      0.77      0.82       293\n",
        "         1.0       0.80      0.88      0.84       300\n",
        "\n",
        "   micro avg       0.83      0.83      0.83       593\n",
        "   macro avg       0.83      0.83      0.83       593\n",
        "weighted avg       0.83      0.83      0.83       593\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.71      0.74      0.73       293\n",
        "         1.0       0.74      0.71      0.72       300\n",
        "\n",
        "   micro avg       0.73      0.73      0.73       593\n",
        "   macro avg       0.73      0.73      0.73       593\n",
        "weighted avg       0.73      0.73      0.73       593\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT3SqXpcraIa",
        "colab_type": "text"
      },
      "source": [
        "BiLSTM model for Past task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr-5WsgytZtx",
        "colab_type": "text"
      },
      "source": [
        "Stacked BiLSTM for Past task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEvFT3ffs_BU",
        "colab_type": "code",
        "outputId": "025c85ce-1e02-40c3-f704-d08169b91e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "#Past LSTM2\n",
        "session_refresh()\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocabulary_size,300, input_length = MAX_LEN,weights = [embedding_matrix],trainable = False))\n",
        "# model.add(Embedding(vocabulary_size, 145, input_length=145))\n",
        "model2.add(Bidirectional(LSTM(145, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
        "model2.add(Bidirectional(LSTM(145, dropout=0.1, recurrent_dropout=0.1)))\n",
        "model2.add(Dense(64,activation = 'relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])\n",
        "# model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "filepath=datapath+'/models/'+\"past-2-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model2.summary()\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "refreshed session\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 290, 300)          2031300   \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 290, 290)          517360    \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 290)               505760    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                18624     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,073,109\n",
            "Trainable params: 1,041,809\n",
            "Non-trainable params: 2,031,300\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRFhkekKtEWg",
        "colab_type": "code",
        "outputId": "a9cd7126-fbee-4c45-b194-0a1f63ddb5a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "model2.fit(tr_data, tr_pt, validation_data=(val_data,val_pt), epochs=10,callbacks=callbacks_list,batch_size=64) #eposh 8 batch 128 no relu hidden\n",
        "model2_json = model2.to_json()\n",
        "with open(datapath+\"past-2-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model2_json)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1776 samples, validate on 592 samples\n",
            "Epoch 1/10\n",
            "1776/1776 [==============================] - 54s 31ms/step - loss: 0.6876 - acc: 0.6053 - val_loss: 0.6041 - val_acc: 0.6740\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.67399, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 2/10\n",
            "1776/1776 [==============================] - 48s 27ms/step - loss: 0.6368 - acc: 0.6678 - val_loss: 0.5584 - val_acc: 0.7416\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.67399 to 0.74155, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 3/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.5412 - acc: 0.7545 - val_loss: 0.5322 - val_acc: 0.7568\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.74155 to 0.75676, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 4/10\n",
            "1776/1776 [==============================] - 51s 29ms/step - loss: 0.4852 - acc: 0.8018 - val_loss: 0.5492 - val_acc: 0.7635\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.75676 to 0.76351, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 5/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.4628 - acc: 0.7934 - val_loss: 0.4866 - val_acc: 0.7753\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.76351 to 0.77534, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 6/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.4502 - acc: 0.8080 - val_loss: 0.4832 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.77534 to 0.79054, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 7/10\n",
            "1776/1776 [==============================] - 49s 28ms/step - loss: 0.4183 - acc: 0.8322 - val_loss: 0.4808 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.79054\n",
            "Epoch 8/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.3884 - acc: 0.8423 - val_loss: 0.4761 - val_acc: 0.7838\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.79054\n",
            "Epoch 9/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.4000 - acc: 0.8345 - val_loss: 0.4918 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.79054\n",
            "Epoch 10/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.3795 - acc: 0.8480 - val_loss: 0.4873 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.79054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUkfWfaoso_3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCl9GRqIrKUo",
        "colab_type": "code",
        "outputId": "28a94726-f46f-4108-fd3d-340ec4009f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# model2.fit(np.concatenate((tr_data,val_data)), np.concatenate((tr_pt,val_pt)), epochs=9,batch_size=64)\n",
        "# model2_json = model2.to_json()\n",
        "# with open(datapath+\"past-2-full-model.json\", \"w\") as json_file:\n",
        "#     json_file.write(model2_json)\n",
        "# model2.save_weights(datapath+'/models/'+\"past-2-full-best.hdf5\")\n",
        "# print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "2368/2368 [==============================] - 37s 15ms/step - loss: 0.3353 - acc: 0.8611\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7T_wki-hVru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "338d568d-fbc1-40bd-aa19-18e4d5c64a56"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "# load json and create model\n",
        "json_file = open(datapath+\"past-2-model.json\", 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model2 = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "model2.load_weights(datapath+'/models/'+\"past-2-best.hdf5\")\n",
        "print(\"Loaded model from disk\")\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAS0PMF6tuUt",
        "colab_type": "text"
      },
      "source": [
        "Evaluating the test data for future task "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2ZDFnFzg8KE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEYgaCrPtjdg",
        "colab_type": "code",
        "outputId": "208b8f47-734c-4392-f032-8212c02b1bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores = model2.evaluate(ts_data, ts_pt, verbose=0)\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 72.34%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNYbtkOojYCL",
        "colab_type": "text"
      },
      "source": [
        "Accuracy: 78.75%\n",
        "\n",
        "\n",
        "w/pos: Accuracy: 72.34%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5egYg3jSjysF",
        "colab_type": "code",
        "outputId": "75eb61c6-4035-4636-e5b3-5125c42bb6f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import model_from_json\n",
        "# load json and create model\n",
        "json_file = open(datapath+\"past-2-full-model.json\", 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model2 = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "model2.load_weights(datapath+'/models/'+\"past-2-full-best.hdf5\")\n",
        "print(\"Loaded model from disk\")\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1XpIdlStxPo",
        "colab_type": "text"
      },
      "source": [
        "Generating a classification report for the test data for future task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RAmOxrbtrIh",
        "colab_type": "code",
        "outputId": "7313cb60-3b9f-4fd3-a207-1ce70ded1793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred = model2.predict(ts_data, batch_size=64, verbose=1)\n",
        "y_pred =(y_pred>0.5)\n",
        "\n",
        "print(classification_report(ts_pt, y_pred))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "593/593 [==============================] - 7s 12ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.96      0.79       324\n",
            "         1.0       0.90      0.44      0.59       269\n",
            "\n",
            "   micro avg       0.72      0.72      0.72       593\n",
            "   macro avg       0.79      0.70      0.69       593\n",
            "weighted avg       0.78      0.72      0.70       593\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MYdGOnwk3ps",
        "colab_type": "text"
      },
      "source": [
        "Past Test Accuracy: 78.75% ; epochs 9, model: BiLSTM + Dense\n",
        "\n",
        "precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.78      0.85      0.81       324\n",
        "         1.0       0.80      0.71      0.75       269\n",
        "\n",
        "   micro avg       0.79      0.79      0.79       593\n",
        "   \n",
        "   macro avg       0.79      0.78      0.78       593\n",
        "   \n",
        "weighted avg       0.79      0.79      0.79       593\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpxoEmpnwvY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}