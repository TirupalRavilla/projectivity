{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projectivity LSTM Glove .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9GM_TH5roB0",
        "colab_type": "text"
      },
      "source": [
        "Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu6pel2nWGBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import linear_model, preprocessing\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re \n",
        "# nltk.download('stopwords')\n",
        "# nltk.download()\n",
        "from nltk.corpus import stopwords "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DUffYSxtJl_",
        "colab_type": "code",
        "outputId": "2fbdb089-3003-440f-9fcc-694e25da7b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4183
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('all')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxZrfr29nAm3",
        "colab_type": "text"
      },
      "source": [
        "Compile and run from here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIVbzDgMnIHH",
        "colab_type": "text"
      },
      "source": [
        "To load the train and validation data from TSV files.\n",
        "\n",
        "Arguments: train file, validation file\n",
        "\n",
        "Returns:\n",
        "\n",
        "tr_st: Training sentences\n",
        "tr_ft: Training Future Labels\n",
        "tr_pt: Trainiing past labels\n",
        "val_st: Validation sentences\n",
        "val_ft: Validation Future Labels\n",
        "val_pt: Validation past labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhIpVrntXle-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_train_val(train,val):\n",
        "  df= pd.read_csv(train,delimiter='\\t',encoding='utf-8')\n",
        "  tr_st=np.array(df['sentence'].values)\n",
        "  tr_ft=np.array(df['Future'].values)\n",
        "  tr_pt=np.array(df['Past'].values)\n",
        "  df= pd.read_csv(val,delimiter='\\t',encoding='utf-8')\n",
        "  val_st=np.array(df['sentence'].values)\n",
        "  val_ft=np.array(df['Future'].values)\n",
        "  val_pt=np.array(df['Past'].values)\n",
        "  return tr_st,tr_ft,tr_pt, val_st,val_ft,val_pt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZFGdfLUm_-d",
        "colab_type": "text"
      },
      "source": [
        "Method to load the test data from TSV file.\n",
        "\n",
        "Arguments: test file\n",
        "\n",
        "Returns:\n",
        "\n",
        "ts_st: Test sentences\n",
        "ts_ft: Test Future Labels\n",
        "ts_pt: Test past labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxxZx8cfjN4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_test(test):\n",
        "  df= pd.read_csv(test,delimiter='\\t',encoding='utf-8')\n",
        "  ts_st=np.array(df['sentence'].values)\n",
        "  ts_ft=np.array(df['Future'].values)\n",
        "  ts_pt=np.array(df['Past'].values)\n",
        "  return ts_st,ts_ft,ts_pt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z8FXCuHoRNM",
        "colab_type": "text"
      },
      "source": [
        "To preprocess sentences:\n",
        "Removing some special characters, spaces and tabs, and conversion to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olOQd2m9VDBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentences(X):\n",
        "  documents = []\n",
        "  stemmer = WordNetLemmatizer()\n",
        "  for sen in range(0, len(X)):\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    document = re.sub(r'^\"', '', document)\n",
        "    document = re.sub(r'\"$', '', document)\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    document = document.lower()\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    #     document = ' '.join(document)\n",
        "    documents.append(document)\n",
        "  print(len(documents))\n",
        "  return documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUjEdlLqoju2",
        "colab_type": "text"
      },
      "source": [
        "Converts sentences to lists of features\n",
        "\n",
        "Uncomment code as needed for features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQlVvKEebN01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_features(documents):\n",
        "  feature_docs=[]\n",
        "  for document in documents:\n",
        "    tokens=nltk.word_tokenize(document)\n",
        "    bigrams = list(nltk.bigrams(tokens))\n",
        "    #pos_bigrams = list(nltk.bigrams([pos for (word,pos) in nltk.pos_tag(tokens)]))\n",
        "    document=[word for (word,pos) in nltk.pos_tag(tokens)]#+' '+pos\n",
        "#     document.extend([w1+'_'+w2 for (w1,w2) in bigrams])\n",
        "    #     document.extend([w1+'_'+w2 for (w1,w2) in pos_bigrams])\n",
        "    #     document.extend([pos for (word,pos) in nltk.pos_tag(tokens)])\n",
        "    #     document = document.split()\n",
        "    #     document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    feature_docs.append(document)\n",
        "  return feature_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iQzX5Oyo6hl",
        "colab_type": "text"
      },
      "source": [
        "This is not necesasry unless the code is being run on a Colab notebook, and the data is stored on Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYP9853kPwtg",
        "colab_type": "code",
        "outputId": "4a4b7a5d-afb3-4bf3-808e-c0ceb97a9427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmgo2XqVpNL1",
        "colab_type": "text"
      },
      "source": [
        "Path to the location where the data resides"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKhqgC5FQHyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapath = 'gdrive/My Drive/data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "westpD8MpWDa",
        "colab_type": "text"
      },
      "source": [
        "Loading the train and validation data\n",
        "\n",
        "Preprocessing the data\n",
        "\n",
        "Feature extraction if necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjfiO4FeJz6",
        "colab_type": "code",
        "outputId": "2d496be0-ddec-4bc2-c304-5ccf12b335f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#loading train and validation data\n",
        "trainfile=datapath+'train.txt'\n",
        "valfile=datapath+'validation.txt'\n",
        "tr_st,tr_ft,tr_pt, val_st,val_ft,val_pt=load_train_val(trainfile,valfile)\n",
        "\n",
        "#preprocessing\n",
        "tr_st = preprocess_sentences(tr_st)\n",
        "val_st = preprocess_sentences(val_st)\n",
        "\n",
        "#feature extraction\n",
        "tr_st = sent_to_features(tr_st)\n",
        "val_st = sent_to_features(val_st)\n",
        "\n",
        "print('Train size: ',len(tr_st))\n",
        "print('Val size: ',len(val_st))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1776\n",
            "592\n",
            "Train size:  1776\n",
            "Val size:  592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUwpVDvpCT7n",
        "colab_type": "code",
        "outputId": "39f4dc04-c439-415f-b4dc-cf52043f2887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tr_st[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "friends and adversaries abroad were asking whether america had lost its nerve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhyHmoYhpl5V",
        "colab_type": "text"
      },
      "source": [
        "Imports necessary for LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VngaIqK5HAoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, CuDNNLSTM, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import optimizers\n",
        "\n",
        "# from tensorflow.keras.backend import set_session\n",
        "import keras.backend as K\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2DL356Ypscx",
        "colab_type": "text"
      },
      "source": [
        "Creating a Keras Tokenizer and fitting it on train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LFo1Iwafnvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(np.concatenate((tr_st,val_st)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV3xBHu2se9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "pos_model = Word2Vec.load(datapath+'model_300d.bin')\n",
        "# print(pos_model)\n",
        "words = list(pos_model.wv.vocab)\n",
        "# print(words)\n",
        "# print(len(words))\n",
        "# print(pos_model['NN'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrBOaXLzqAsA",
        "colab_type": "text"
      },
      "source": [
        "Initializing vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flu9dPuMfuuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size = len(tokenizer.word_index)+1\n",
        "# print(tokenizer.word_counts)\n",
        "# print(tokenizer.document_count)\n",
        "# print(tokenizer.word_index)\n",
        "# print(tokenizer.word_docs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh-cLMCvihEX",
        "colab_type": "code",
        "outputId": "6db71041-b3be-459c-80c1-2ff80b8a5e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(vocabulary_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ekEbMnItX59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #feature extraction\n",
        "# tr_st = sent_to_features(tr_st)\n",
        "# val_st = sent_to_features(val_st)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_H4rwCgqT9Y",
        "colab_type": "text"
      },
      "source": [
        "Extracting sequences from train and validation data, and padding them to a maximum length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrQ-RHoOG7Jf",
        "colab_type": "code",
        "outputId": "55bc4789-4e7b-468e-be44-e80550ec17df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "# vocabulary_size = 7000\n",
        "# tokenizer.fit_on_texts(np.concatenate((tr_st,val_st)))\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(tr_st)\n",
        "tr_data = pad_sequences(sequences, maxlen=145)\n",
        "sequences = tokenizer.texts_to_sequences(val_st)\n",
        "val_data = pad_sequences(sequences, maxlen=145)\n",
        "\n",
        "print(len(tr_data))\n",
        "print(len(val_data))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1776\n",
            "592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaPP6ZFFqf05",
        "colab_type": "text"
      },
      "source": [
        "Loading Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGKoxGURzgxb",
        "colab_type": "code",
        "outputId": "a72ae3a2-8815-44e5-eb4f-745d611e186c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f = open(datapath+'glove.6B.300d.txt')\n",
        "embedding_values = {}\n",
        "for line in tqdm(f):\n",
        "  value = line.split(' ')\n",
        "  word = value[0]\n",
        "  coef = np.array(value[1:],dtype = 'float32')\n",
        "  embedding_values[word]=coef"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000it [00:36, 10928.29it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFY6sYIJtfxd",
        "colab_type": "text"
      },
      "source": [
        "Adding embedding values of POS tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y121AORfjwmJ",
        "colab_type": "code",
        "outputId": "5859c665-3b6c-46c3-b319-c40a18ccf5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "for word in words:\n",
        "  coef=np.array(pos_model[word],dtype = 'float32')\n",
        "  embedding_values[word]=coef"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4naBxGnFuoCh",
        "colab_type": "code",
        "outputId": "062ca6f1-9ed2-4e2f-a70f-2006a5a5d9aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(embedding_values)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNBfnm5eqmLe",
        "colab_type": "text"
      },
      "source": [
        "Creating an embedding matrix for words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu234t4CzsfJ",
        "colab_type": "code",
        "outputId": "f5e4cd26-8d3a-4ed7-b4e3-0572ddf58ebd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_matrix = np.zeros((vocabulary_size,300))\n",
        "for word,i in tqdm(tokenizer.word_index.items()):\n",
        "  values = embedding_values.get(word)\n",
        "  if values is not None:\n",
        "    embedding_matrix[i] = values"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6743/6743 [00:00<00:00, 272090.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Y21Y9aq5ig",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8vIKkokx2Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "from numpy.random import seed\n",
        "from tensorflow import set_random_seed\n",
        "\n",
        "\n",
        "seed(2)\n",
        "set_random_seed(2)\n",
        "rn.seed(2)\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRHaXxzKcY_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def session_refresh():\n",
        "  seed(2)\n",
        "  set_random_seed(2)\n",
        "  rn.seed(2)\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "  print('refreshed session')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bBpZbINq9If",
        "colab_type": "text"
      },
      "source": [
        "BiLSTM for Future task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoL3XrfjKYfY",
        "colab_type": "code",
        "outputId": "f87841b7-12be-442e-c25a-13694fc43844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        }
      },
      "source": [
        "#Future\n",
        "# K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size,300, input_length = 145,weights = [embedding_matrix],trainable = False))\n",
        "# model.add(Embedding(vocabulary_size, 145, input_length=145))\n",
        "model.add(Bidirectional(LSTM(145, dropout=0.2, recurrent_dropout=0.2)))\n",
        "\n",
        "model.add(Dense(64,activation = 'relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])\n",
        "\n",
        "filepath=datapath+'/models/'+\"future-1-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.summary()\n",
        "model.fit(tr_data, tr_ft, validation_data=(val_data,val_ft), epochs=10,callbacks=callbacks_list,batch_size=64)\n",
        "model_json = model.to_json()\n",
        "with open(datapath+\"future-1-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# model.fit(np.concatenate((tr_data,val_data)), np.concatenate((tr_ft,val_ft)), epochs=10,batch_size=64)#  validation_data=(val_data,val_ft),"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 145, 300)          2023200   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 290)               517360    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                18624     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,559,249\n",
            "Trainable params: 536,049\n",
            "Non-trainable params: 2,023,200\n",
            "_________________________________________________________________\n",
            "Train on 1776 samples, validate on 592 samples\n",
            "Epoch 1/10\n",
            "1776/1776 [==============================] - 17s 10ms/step - loss: 0.7113 - acc: 0.6120 - val_loss: 0.5403 - val_acc: 0.7297\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.72973, saving model to gdrive/My Drive/data//models/future-1-best.hdf5\n",
            "Epoch 2/10\n",
            "1776/1776 [==============================] - 14s 8ms/step - loss: 0.5385 - acc: 0.7427 - val_loss: 0.5273 - val_acc: 0.7449\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.72973 to 0.74493, saving model to gdrive/My Drive/data//models/future-1-best.hdf5\n",
            "Epoch 3/10\n",
            "1776/1776 [==============================] - 14s 8ms/step - loss: 0.4859 - acc: 0.7776 - val_loss: 0.4725 - val_acc: 0.7855\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.74493 to 0.78547, saving model to gdrive/My Drive/data//models/future-1-best.hdf5\n",
            "Epoch 4/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.4392 - acc: 0.8057 - val_loss: 0.4897 - val_acc: 0.7686\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.78547\n",
            "Epoch 5/10\n",
            "1776/1776 [==============================] - 14s 8ms/step - loss: 0.4084 - acc: 0.8209 - val_loss: 0.4876 - val_acc: 0.7872\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.78547 to 0.78716, saving model to gdrive/My Drive/data//models/future-1-best.hdf5\n",
            "Epoch 6/10\n",
            "1776/1776 [==============================] - 14s 8ms/step - loss: 0.3686 - acc: 0.8407 - val_loss: 0.5015 - val_acc: 0.8041\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.78716 to 0.80405, saving model to gdrive/My Drive/data//models/future-1-best.hdf5\n",
            "Epoch 7/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.3459 - acc: 0.8514 - val_loss: 0.4573 - val_acc: 0.8007\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.80405\n",
            "Epoch 8/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.3161 - acc: 0.8660 - val_loss: 0.4944 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.80405 to 0.81081, saving model to gdrive/My Drive/data//models/future-1-best.hdf5\n",
            "Epoch 9/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.2956 - acc: 0.8699 - val_loss: 0.4669 - val_acc: 0.7973\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.81081\n",
            "Epoch 10/10\n",
            "1776/1776 [==============================] - 14s 8ms/step - loss: 0.2733 - acc: 0.8789 - val_loss: 0.4810 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.81081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoDUaHXHrD8K",
        "colab_type": "text"
      },
      "source": [
        "Stacked BiLSTM for Future task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIAM23tnQ2_m",
        "colab_type": "code",
        "outputId": "57d605ec-95cb-4544-b406-790f06207ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "#Future LSTM2\n",
        "# K.clear_session()\n",
        "K.set_session(sess)\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocabulary_size,300, input_length = 145,weights = [embedding_matrix],trainable = False))\n",
        "# model.add(Embedding(vocabulary_size, 145, input_length=145))\n",
        "model2.add(Bidirectional(LSTM(145, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
        "model2.add(Bidirectional(LSTM(145, dropout=0.1, recurrent_dropout=0.1)))\n",
        "model2.add(Dense(64,activation = 'relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "\n",
        "filepath=datapath+'/models/'+\"future-2-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])\n",
        "# model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model2.summary()\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 145, 300)          2023200   \n",
            "_________________________________________________________________\n",
            "bidirectional_19 (Bidirectio (None, 145, 290)          517360    \n",
            "_________________________________________________________________\n",
            "bidirectional_20 (Bidirectio (None, 290)               505760    \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 64)                18624     \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,065,009\n",
            "Trainable params: 1,041,809\n",
            "Non-trainable params: 2,023,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJT0QXx3SI1C",
        "colab_type": "code",
        "outputId": "13c53ff5-a711-46af-eb96-d6481cbe5319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "model2.fit(tr_data, tr_ft, validation_data=(val_data,val_ft), epochs=10,callbacks=callbacks_list,batch_size=64)  #epoch 15 batch 128 no relu hidden   epoch 7 batch 128 w/ relu hidden\n",
        "model_json = model.to_json()\n",
        "with open(datapath+\"future-2-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1776 samples, validate on 592 samples\n",
            "Epoch 1/10\n",
            "1776/1776 [==============================] - 38s 21ms/step - loss: 0.7058 - acc: 0.5822 - val_loss: 0.5670 - val_acc: 0.7416\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.74155, saving model to gdrive/My Drive/data//models/future-2-best.hdf5\n",
            "Epoch 2/10\n",
            "1776/1776 [==============================] - 29s 16ms/step - loss: 0.5436 - acc: 0.7461 - val_loss: 0.4646 - val_acc: 0.7939\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.74155 to 0.79392, saving model to gdrive/My Drive/data//models/future-2-best.hdf5\n",
            "Epoch 3/10\n",
            "1776/1776 [==============================] - 30s 17ms/step - loss: 0.4981 - acc: 0.7866 - val_loss: 0.4459 - val_acc: 0.7939\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.79392\n",
            "Epoch 4/10\n",
            "1776/1776 [==============================] - 30s 17ms/step - loss: 0.4542 - acc: 0.7967 - val_loss: 0.4508 - val_acc: 0.8007\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.79392 to 0.80068, saving model to gdrive/My Drive/data//models/future-2-best.hdf5\n",
            "Epoch 5/10\n",
            "1776/1776 [==============================] - 30s 17ms/step - loss: 0.4268 - acc: 0.8131 - val_loss: 0.4231 - val_acc: 0.8176\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.80068 to 0.81757, saving model to gdrive/My Drive/data//models/future-2-best.hdf5\n",
            "Epoch 6/10\n",
            "1776/1776 [==============================] - 31s 17ms/step - loss: 0.3977 - acc: 0.8294 - val_loss: 0.4579 - val_acc: 0.7973\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.81757\n",
            "Epoch 7/10\n",
            "1776/1776 [==============================] - 30s 17ms/step - loss: 0.3825 - acc: 0.8356 - val_loss: 0.4601 - val_acc: 0.8142\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.81757\n",
            "Epoch 8/10\n",
            "1776/1776 [==============================] - 31s 17ms/step - loss: 0.3580 - acc: 0.8412 - val_loss: 0.4523 - val_acc: 0.8024\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.81757\n",
            "Epoch 9/10\n",
            "1776/1776 [==============================] - 30s 17ms/step - loss: 0.3241 - acc: 0.8587 - val_loss: 0.4793 - val_acc: 0.8142\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.81757\n",
            "Epoch 10/10\n",
            "1776/1776 [==============================] - 30s 17ms/step - loss: 0.3064 - acc: 0.8722 - val_loss: 0.5512 - val_acc: 0.8041\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.81757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvOsRrrBVrzW",
        "colab_type": "text"
      },
      "source": [
        "Train on 1776 samples, validate on 592 samples\n",
        "Epoch 1/10\n",
        "1776/1776 [==============================] - 62s 35ms/step - loss: 0.6709 - acc: 0.6267 - val_loss: 0.5295 - val_acc: 0.7686\n",
        "Epoch 2/10\n",
        "1776/1776 [==============================] - 56s 32ms/step - loss: 0.5727 - acc: 0.7230 - val_loss: 0.5095 - val_acc: 0.7618\n",
        "Epoch 3/10\n",
        "1776/1776 [==============================] - 57s 32ms/step - loss: 0.5068 - acc: 0.7742 - val_loss: 0.4501 - val_acc: 0.7872\n",
        "Epoch 4/10\n",
        "1776/1776 [==============================] - 57s 32ms/step - loss: 0.4625 - acc: 0.7922 - val_loss: 0.4338 - val_acc: 0.8074\n",
        "Epoch 5/10\n",
        "1776/1776 [==============================] - 58s 32ms/step - loss: 0.4359 - acc: 0.8012 - val_loss: 0.4197 - val_acc: 0.8057\n",
        "Epoch 6/10\n",
        "1776/1776 [==============================] - 57s 32ms/step - loss: 0.3968 - acc: 0.8294 - val_loss: 0.4183 - val_acc: 0.8243\n",
        "Epoch 7/10\n",
        "1776/1776 [==============================] - 58s 32ms/step - loss: 0.3847 - acc: 0.8249 - val_loss: 0.4313 - val_acc: 0.8108\n",
        "Epoch 8/10\n",
        "1776/1776 [==============================] - 58s 32ms/step - loss: 0.3585 - acc: 0.8457 - val_loss: 0.4272 - val_acc: 0.8091\n",
        "Epoch 9/10\n",
        "1776/1776 [==============================] - 56s 32ms/step - loss: 0.3308 - acc: 0.8581 - val_loss: 0.4347 - val_acc: 0.8091\n",
        "Epoch 10/10\n",
        "1776/1776 [==============================] - 56s 32ms/step - loss: 0.3063 - acc: 0.8632 - val_loss: 0.4721 - val_acc: 0.7787"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR8wePLRrNg7",
        "colab_type": "text"
      },
      "source": [
        "Training on whole training data after parameter tuning, in order to use the model to evaluate the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-IS4iu57ltx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "dc897642-c55b-4b52-c535-c0a87719fbf8"
      },
      "source": [
        "model2.fit(np.concatenate((tr_data,val_data)), np.concatenate((tr_ft,val_ft)), epochs=3,batch_size=64)\n",
        "\n",
        "model_json = model2.to_json()\n",
        "with open(datapath+'/models/'+\"future-2-full-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model2.save_weights(datapath+'/models/'+\"future-2-full-best.hdf5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "2368/2368 [==============================] - 37s 16ms/step - loss: 0.3892 - acc: 0.8247\n",
            "Epoch 2/3\n",
            "2368/2368 [==============================] - 36s 15ms/step - loss: 0.3542 - acc: 0.8429\n",
            "Epoch 3/3\n",
            "2368/2368 [==============================] - 36s 15ms/step - loss: 0.3433 - acc: 0.8514\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZWGyo6Gt_Vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI9HnP6YriTJ",
        "colab_type": "text"
      },
      "source": [
        "Loading the Test data\n",
        "\n",
        "preprocessing the data\n",
        "\n",
        "Extracting sequences\n",
        "\n",
        "Padding the sequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om3T_CwSMj_R",
        "colab_type": "code",
        "outputId": "ba304f16-0789-49a5-bc7f-823d487ba47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "testfile=datapath+'test.txt'\n",
        "ts_st,ts_ft,ts_pt=load_test(testfile)\n",
        "\n",
        "#preprocessing\n",
        "ts_st = preprocess_sentences(ts_st)\n",
        "\n",
        "#feature extraction\n",
        "#ts_st = sent_to_features(ts_st)\n",
        "sequences = tokenizer.texts_to_sequences(ts_st)\n",
        "ts_data = pad_sequences(sequences, maxlen=145)\n",
        "print(ts_data.shape)\n",
        "print(ts_ft.shape)\n",
        "# test_loss, test_acc = model.evaluate(np.append((ts_data,ts_ft),1))\n",
        "# print('Test Loss: {}'.format(test_loss))\n",
        "# print('Test Accuracy: {}'.format(test_acc))\n",
        "\n",
        "\n",
        "# print('Accuaracy = {0:f}'.format(acc))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "593\n",
            "(593, 145)\n",
            "(593,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ0j2FCCLkkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFXas8tJscK5",
        "colab_type": "text"
      },
      "source": [
        "Evaluating the test data for future task "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvgVJ-NcsbRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a942df83-8e96-41fc-fb40-4fff2cb3c4e1"
      },
      "source": [
        "\n",
        "scores = model2.evaluate(ts_data, ts_ft, verbose=0)\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 82.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69CS2ezdWyoy",
        "colab_type": "text"
      },
      "source": [
        "Future Test Accuracy: 82.80% ; epochs 5+3, model: BiLSTM + Dense\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PpAnA1ff52k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65e399a2-bb63-4cd3-f91b-dcb2434a4c96"
      },
      "source": [
        "# testf='sotu_2019.tsv'\n",
        "# df= pd.read_csv(testf,delimiter='\\t',encoding='utf-8')\n",
        "# full_st=np.array(df['sentence'].values)\n",
        "# full_st = preprocess_sentences(full_st)\n",
        "\n",
        "# #feature extraction\n",
        "# #ts_st = sent_to_features(ts_st)\n",
        "# sequences = tokenizer.texts_to_sequences(full_st)\n",
        "# full_st_data = pad_sequences(sequences, maxlen=145)\n",
        "\n",
        "\n",
        "# y_pred = model2.predict(full_st_data, batch_size=64, verbose=1)\n",
        "# y_pred =(y_pred>0.5)\n",
        "\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 4s 9ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7ybqvXgqOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df['ft']=list(map(int,y_pred))\n",
        "# df.to_csv('sotu_future.tsv',sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-sctPFuQaeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "# model_json = model2.to_json()\n",
        "# with open(datapath+\"future_model2.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "# # serialize weights to HDF5\n",
        "# model2.save_weights(datapath+\"future_model2.h5\")\n",
        "# print(\"Saved model to disk\")\n",
        " \n",
        "# # later...\n",
        " \n",
        "# # load json and create model\n",
        "# json_file = open('model.json', 'r')\n",
        "# loaded_model_json = json_file.read()\n",
        "# json_file.close()\n",
        "# loaded_model = model_from_json(loaded_model_json)\n",
        "# # load weights into new model\n",
        "# loaded_model.load_weights(\"model.h5\")\n",
        "# print(\"Loaded model from disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW3UEFHSsjeE",
        "colab_type": "text"
      },
      "source": [
        "Generating a classification report for the test data for future task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnKBdfMv6ezo",
        "colab_type": "code",
        "outputId": "8f93de9d-38c4-4239-bdc8-08a78094ba3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred = model2.predict(ts_data, batch_size=64, verbose=1)\n",
        "y_pred =(y_pred>0.5)\n",
        "\n",
        "print(classification_report(ts_ft, y_pred))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "593/593 [==============================] - 7s 12ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.77      0.82       293\n",
            "         1.0       0.80      0.88      0.84       300\n",
            "\n",
            "   micro avg       0.83      0.83      0.83       593\n",
            "   macro avg       0.83      0.83      0.83       593\n",
            "weighted avg       0.83      0.83      0.83       593\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5H5yWnhXf7l",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Future Test Accuracy: 82.80% ; epochs 5+3, model: BiLSTM + Dense\n",
        "\n",
        "precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.87      0.77      0.82       293\n",
        "         1.0       0.80      0.88      0.84       300\n",
        "\n",
        "   micro avg       0.83      0.83      0.83       593\n",
        "   macro avg       0.83      0.83      0.83       593\n",
        "weighted avg       0.83      0.83      0.83       593\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6-ArKGc84mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scores = model2.evaluate(ts_data, ts_ft, verbose=0)\n",
        "\n",
        "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "# y_pred = model2.predict(ts_data, batch_size=64, verbose=1)\n",
        "# y_pred =(y_pred>0.5)\n",
        "\n",
        "# print(classification_report(ts_ft, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7jQDXMYXvbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT3SqXpcraIa",
        "colab_type": "text"
      },
      "source": [
        "BiLSTM model for Past task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb1lQ0D1Gy4N",
        "colab_type": "code",
        "outputId": "3559bd82-46d6-4673-8ffb-0393f091ee93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        }
      },
      "source": [
        "#Past\n",
        "\n",
        "# K.clear_session()\n",
        "K.set_session(sess)\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size,300, input_length = 145,weights = [embedding_matrix],trainable = False))\n",
        "# model.add(Embedding(vocabulary_size, 145, input_length=145))\n",
        "model.add(Bidirectional(LSTM(145, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(64,activation = 'relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])\n",
        "\n",
        "filepath=datapath+'/models/'+\"past-1-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "model.summary()\n",
        "model.fit(tr_data, tr_pt, validation_data=(val_data,val_pt), epochs=10,callbacks=callbacks_list,batch_size=64) #eposh 8 batch 128 no relu hidden\n",
        "model_json = model.to_json()\n",
        "with open(datapath+\"past-1-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 145, 300)          2023200   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 290)               517360    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                18624     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,559,249\n",
            "Trainable params: 536,049\n",
            "Non-trainable params: 2,023,200\n",
            "_________________________________________________________________\n",
            "Train on 1776 samples, validate on 592 samples\n",
            "Epoch 1/10\n",
            "1776/1776 [==============================] - 18s 10ms/step - loss: 0.6555 - acc: 0.6329 - val_loss: 0.5829 - val_acc: 0.7162\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.71622, saving model to gdrive/My Drive/data//models/past-1-best.hdf5\n",
            "Epoch 2/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.4978 - acc: 0.7810 - val_loss: 0.4737 - val_acc: 0.7973\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.71622 to 0.79730, saving model to gdrive/My Drive/data//models/past-1-best.hdf5\n",
            "Epoch 3/10\n",
            "1776/1776 [==============================] - 16s 9ms/step - loss: 0.4693 - acc: 0.7979 - val_loss: 0.5181 - val_acc: 0.7855\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.79730\n",
            "Epoch 4/10\n",
            "1776/1776 [==============================] - 15s 9ms/step - loss: 0.4260 - acc: 0.8215 - val_loss: 0.4856 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.79730\n",
            "Epoch 5/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.3988 - acc: 0.8339 - val_loss: 0.4846 - val_acc: 0.8041\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.79730 to 0.80405, saving model to gdrive/My Drive/data//models/past-1-best.hdf5\n",
            "Epoch 6/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.3762 - acc: 0.8373 - val_loss: 0.4807 - val_acc: 0.8007\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.80405\n",
            "Epoch 7/10\n",
            "1776/1776 [==============================] - 14s 8ms/step - loss: 0.3586 - acc: 0.8407 - val_loss: 0.4934 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.80405\n",
            "Epoch 8/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.3643 - acc: 0.8480 - val_loss: 0.4929 - val_acc: 0.7973\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.80405\n",
            "Epoch 9/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.3213 - acc: 0.8660 - val_loss: 0.5207 - val_acc: 0.8007\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.80405\n",
            "Epoch 10/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.3076 - acc: 0.8767 - val_loss: 0.5204 - val_acc: 0.7939\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.80405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_LvKy9DG3Km",
        "colab_type": "code",
        "outputId": "63951963-d0b0-40aa-805c-c0ef48a238dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        }
      },
      "source": [
        "\n",
        "# K.clear_session()\n",
        "session_refresh()\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size,300, input_length = 145,weights = [embedding_matrix],trainable = False))\n",
        "# model.add(Embedding(vocabulary_size, 145, input_length=145))\n",
        "model.add(Bidirectional(LSTM(145, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(64,activation = 'relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "filepath=datapath+'/models/'+\"past-1-adam-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "model.summary()\n",
        "model.fit(tr_data, tr_pt, validation_data=(val_data,val_pt), epochs=10,callbacks=callbacks_list,batch_size=64) #eposh 8 batch 128 no relu hidden\n",
        "model_json = model.to_json()\n",
        "with open(datapath+\"past-1-adam-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "refreshed session\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 145, 300)          2023200   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 290)               517360    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                18624     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,559,249\n",
            "Trainable params: 536,049\n",
            "Non-trainable params: 2,023,200\n",
            "_________________________________________________________________\n",
            "Train on 1776 samples, validate on 592 samples\n",
            "Epoch 1/10\n",
            "1776/1776 [==============================] - 18s 10ms/step - loss: 0.6387 - acc: 0.6334 - val_loss: 0.5910 - val_acc: 0.6892\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.68919, saving model to gdrive/My Drive/data//models/past-1-adam-best.hdf5\n",
            "Epoch 2/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.5015 - acc: 0.7748 - val_loss: 0.4989 - val_acc: 0.7855\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.68919 to 0.78547, saving model to gdrive/My Drive/data//models/past-1-adam-best.hdf5\n",
            "Epoch 3/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.4559 - acc: 0.8001 - val_loss: 0.5343 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.78547\n",
            "Epoch 4/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.4291 - acc: 0.8311 - val_loss: 0.5254 - val_acc: 0.7652\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.78547\n",
            "Epoch 5/10\n",
            "1776/1776 [==============================] - 14s 8ms/step - loss: 0.4244 - acc: 0.8243 - val_loss: 0.4685 - val_acc: 0.8041\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.78547 to 0.80405, saving model to gdrive/My Drive/data//models/past-1-adam-best.hdf5\n",
            "Epoch 6/10\n",
            "1776/1776 [==============================] - 14s 8ms/step - loss: 0.4088 - acc: 0.8277 - val_loss: 0.5103 - val_acc: 0.7922\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.80405\n",
            "Epoch 7/10\n",
            "1776/1776 [==============================] - 14s 8ms/step - loss: 0.3644 - acc: 0.8497 - val_loss: 0.4793 - val_acc: 0.7922\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.80405\n",
            "Epoch 8/10\n",
            "1776/1776 [==============================] - 16s 9ms/step - loss: 0.3394 - acc: 0.8581 - val_loss: 0.4670 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.80405\n",
            "Epoch 9/10\n",
            "1776/1776 [==============================] - 15s 9ms/step - loss: 0.3394 - acc: 0.8604 - val_loss: 0.4908 - val_acc: 0.7922\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.80405\n",
            "Epoch 10/10\n",
            "1776/1776 [==============================] - 15s 8ms/step - loss: 0.3131 - acc: 0.8750 - val_loss: 0.5083 - val_acc: 0.7855\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.80405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr-5WsgytZtx",
        "colab_type": "text"
      },
      "source": [
        "Stacked BiLSTM for Past task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEvFT3ffs_BU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "b5fe141e-b783-451e-f040-a68fa0f7fc43"
      },
      "source": [
        "#Past LSTM2\n",
        "session_refresh()\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocabulary_size,300, input_length = 145,weights = [embedding_matrix],trainable = False))\n",
        "# model.add(Embedding(vocabulary_size, 145, input_length=145))\n",
        "model2.add(Bidirectional(LSTM(145, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
        "model2.add(Bidirectional(LSTM(145, dropout=0.1, recurrent_dropout=0.1)))\n",
        "model2.add(Dense(64,activation = 'relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])\n",
        "# model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "filepath=datapath+'/models/'+\"past-2-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model2.summary()\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "refreshed session\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 145, 300)          2023200   \n",
            "_________________________________________________________________\n",
            "bidirectional_13 (Bidirectio (None, 145, 290)          517360    \n",
            "_________________________________________________________________\n",
            "bidirectional_14 (Bidirectio (None, 290)               505760    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 64)                18624     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,065,009\n",
            "Trainable params: 1,041,809\n",
            "Non-trainable params: 2,023,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRFhkekKtEWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "464fbb5d-75b8-4100-f9bf-77b45d83525d"
      },
      "source": [
        "model2.fit(tr_data, tr_pt, validation_data=(val_data,val_pt), epochs=10,callbacks=callbacks_list,batch_size=64) #eposh 8 batch 128 no relu hidden\n",
        "model2_json = model2.to_json()\n",
        "with open(datapath+\"past-2-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model2_json)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1776 samples, validate on 592 samples\n",
            "Epoch 1/10\n",
            "1776/1776 [==============================] - 54s 31ms/step - loss: 0.6876 - acc: 0.6053 - val_loss: 0.6041 - val_acc: 0.6740\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.67399, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 2/10\n",
            "1776/1776 [==============================] - 48s 27ms/step - loss: 0.6368 - acc: 0.6678 - val_loss: 0.5584 - val_acc: 0.7416\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.67399 to 0.74155, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 3/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.5412 - acc: 0.7545 - val_loss: 0.5322 - val_acc: 0.7568\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.74155 to 0.75676, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 4/10\n",
            "1776/1776 [==============================] - 51s 29ms/step - loss: 0.4852 - acc: 0.8018 - val_loss: 0.5492 - val_acc: 0.7635\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.75676 to 0.76351, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 5/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.4628 - acc: 0.7934 - val_loss: 0.4866 - val_acc: 0.7753\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.76351 to 0.77534, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 6/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.4502 - acc: 0.8080 - val_loss: 0.4832 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.77534 to 0.79054, saving model to gdrive/My Drive/data//models/past-2-best.hdf5\n",
            "Epoch 7/10\n",
            "1776/1776 [==============================] - 49s 28ms/step - loss: 0.4183 - acc: 0.8322 - val_loss: 0.4808 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.79054\n",
            "Epoch 8/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.3884 - acc: 0.8423 - val_loss: 0.4761 - val_acc: 0.7838\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.79054\n",
            "Epoch 9/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.4000 - acc: 0.8345 - val_loss: 0.4918 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.79054\n",
            "Epoch 10/10\n",
            "1776/1776 [==============================] - 50s 28ms/step - loss: 0.3795 - acc: 0.8480 - val_loss: 0.4873 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.79054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUkfWfaoso_3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCl9GRqIrKUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "28a94726-f46f-4108-fd3d-340ec4009f4a"
      },
      "source": [
        "model2.fit(np.concatenate((tr_data,val_data)), np.concatenate((tr_pt,val_pt)), epochs=9,batch_size=64)\n",
        "model2_json = model2.to_json()\n",
        "with open(datapath+\"past-2-full-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model2_json)\n",
        "model2.save_weights(datapath+'/models/'+\"past-2-full-best.hdf5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "2368/2368 [==============================] - 37s 15ms/step - loss: 0.3353 - acc: 0.8611\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7T_wki-hVru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAS0PMF6tuUt",
        "colab_type": "text"
      },
      "source": [
        "Evaluating the test data for future task "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2ZDFnFzg8KE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEYgaCrPtjdg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef45614d-9a08-4277-da19-c5fd64730dfa"
      },
      "source": [
        "scores = model2.evaluate(ts_data, ts_pt, verbose=0)\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 78.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNYbtkOojYCL",
        "colab_type": "text"
      },
      "source": [
        "Accuracy: 78.75%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTKuP90kwraP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a60fa8f0-b943-4cf7-cfc8-68176daee1a6"
      },
      "source": [
        "# testf=datapath+'sotu_for_analysis.tsv'\n",
        "# df= pd.read_csv(testf,delimiter='\\t',encoding='utf-8')\n",
        "# full_st=np.array(df['sentence'].values)\n",
        "# full_st = preprocess_sentences(full_st)\n",
        "\n",
        "# #feature extraction\n",
        "# #ts_st = sent_to_features(ts_st)\n",
        "# sequences = tokenizer.texts_to_sequences(full_st)\n",
        "# full_st_data = pad_sequences(sequences, maxlen=145)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_pred = model.predict(full_st_data, batch_size=64, verbose=1)\n",
        "y_pred =(y_pred>0.5)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 2s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mbEsIPl2FwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['pt']=list(map(int,y_pred))\n",
        "df.to_csv('sotu_2019_analysis.tsv',sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5egYg3jSjysF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75eb61c6-4035-4636-e5b3-5125c42bb6f2"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "# load json and create model\n",
        "json_file = open(datapath+\"past-2-full-model.json\", 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model2 = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "model2.load_weights(datapath+'/models/'+\"past-2-full-best.hdf5\")\n",
        "print(\"Loaded model from disk\")\n",
        "adg= optimizers.Adagrad(lr=0.01)\n",
        "model2.compile(loss='binary_crossentropy', optimizer=adg, metrics=['accuracy'])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1XpIdlStxPo",
        "colab_type": "text"
      },
      "source": [
        "Generating a classification report for the test data for future task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RAmOxrbtrIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "55e311fe-6490-4e7e-f662-d84b0601b2af"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred = model2.predict(ts_data, batch_size=64, verbose=1)\n",
        "y_pred =(y_pred>0.5)\n",
        "\n",
        "print(classification_report(ts_pt, y_pred))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "593/593 [==============================] - 3s 5ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.85      0.81       324\n",
            "         1.0       0.80      0.71      0.75       269\n",
            "\n",
            "   micro avg       0.79      0.79      0.79       593\n",
            "   macro avg       0.79      0.78      0.78       593\n",
            "weighted avg       0.79      0.79      0.79       593\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCZs-wZS32X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MYdGOnwk3ps",
        "colab_type": "text"
      },
      "source": [
        "Past Test Accuracy: 78.75% ; epochs 9, model: BiLSTM + Dense\n",
        "\n",
        "precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.78      0.85      0.81       324\n",
        "         1.0       0.80      0.71      0.75       269\n",
        "\n",
        "   micro avg       0.79      0.79      0.79       593\n",
        "   \n",
        "   macro avg       0.79      0.78      0.78       593\n",
        "   \n",
        "weighted avg       0.79      0.79      0.79       593\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpxoEmpnwvY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1XMAjVrxuSqV",
        "colab": {}
      },
      "source": [
        "# scores = model2.evaluate(ts_data, ts_pt, verbose=0)\n",
        "\n",
        "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "# y_pred = model2.predict(ts_data, batch_size=64, verbose=1)\n",
        "# y_pred =(y_pred>0.5)\n",
        "\n",
        "# print(classification_report(ts_pt, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}